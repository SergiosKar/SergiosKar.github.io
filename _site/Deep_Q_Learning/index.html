
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Deep Q Learning | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Deep Q Learning" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep Q Learning The journey to Reinforcement learning continues… It’s time to analyze the infamous Q-learning and see how it became the new standard in the field of AI (with a little help from neural networks). First things first. In the last post , we saw the basic concept behind Reinforcement Learning and we frame the problem using an agent, an environment, a state (S), an action(A) and a reward (R). We talked about how the whole process can be described as a Markov Decision Process and we introduced the terms Policy and Value. Lastly, we had a quick high-level overview of the basic methods out there. Remember that the goal is to find the optimal policy and that policy is a mapping between state and actions. So, we need to find which action to take while we stand in a specific state in order to maximize our expected reward. One way to find the optimal policy is to make use of the value functions (a model-free technique). And here we will get to the new stuff. In fact, there are two value functions that are used today. The state value function V(s) and the action value function Q(s, a) . State value function: Is the expected return achieved when acting from a state according to the policy. Action value function: Is the expected return given the state and the action. What is the difference you may ask? The first value is the value of a particular state. The second one is the value of that state plus the values of all the possible actions from that state. When we have the action value function, the Q value, we can simply choose to perform the action with the highest value from a state. But how do we find the Q value?. What is Q learning? So, we will learn the Q value from trial and error? Exactly. We initialize the Q, we choose an action and perform it, we evaluate it by measuring the reward and we update the Q accordingly. In first, randomness will be a key player but as the agent explores the environment, the algorithm will find the best Q value for each state and action. Can we describe this mathematically? Thank you Richard E. Bellmann. The above equation is known as the Bellman equation and plays a huge part in RL today’s research. But what does it state? The Q value, aka the maximum future reward for a state and action, is the immediate reward plus the maximum future reward for the next state. And if you think about it, it makes perfect sense. Gamma (γ) is a number between [0,1] and its used to discount the reward as the time passes, given the assumption that action in the beginning, are more important than at the end (an assumption that is confirmed by many real-life use cases). As a result, we can update the Q value iteratively. The basic concept to understand here is that the Bellman equation relates states with each other and thus, it relates Action value functions. That helps us iterate over the environment and compute the optimal Values, which in turn give us the optimal Policy. In its simplest form, Q values is a matrix with states as rows and actions as columns. We initialize the Q-matrix randomly, the agent starts to interact with the environment and measures the reward for each action. It then computes the observed Q values and updates the matrix. env = gym.make(&#39;MountainCar-v0&#39;) #initalize Q tabe with zeros Q = np.zeros([env.observation_space.n,env.action_space.n]) for i in range(episodes): s = env.reset() reward = 0 goal_flag = False for j in range(200): # greedy action a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) s_new,r,goal_flag,_ = env.step(a)#state and reward maxQ=np.max(Q[s_new,:]) # Belmann Q[s,a] += lr*(r + g*maxQ - Q[s,a]) reward += r s = s_new if goal_flag == True: break Exploration vs Exploitation The algorithm, as described above, is a greedy algorithm, as it always chooses the action with the best value. But what if some action has a very small probability to produce a very large reward? The agent will never get there. This is fixed by adding random exploration. Every once in a while, the agent will perform a random move, without considering the optimal policy. But because we want the algorithm to converge at some point, we lower the probability to take a random action as the game proceeds. Why going Deep? Q learning is good. No one can deny that. But the fact that it is ineffective in big state spaces remains. Imagine a game with 1000 states and 1000 actions per state. We would need a table of 1 million cells. And that is a very small state space comparing to chess or Go. Also, Q learning can’t be used in unknown states because it can’t infer the Q value of new states from the previous ones. What if we approximate the Q values using some machine learning model. What if we approximate them using neural networks? That simple idea (and execution of course) was the reason behind DeepMind acquisition from Google for 500 million dollars. DeepMind proposed an algorithm named Deep Q Learner and used it to play Atari games with impeccable mastery. Deep Q Learning In deep Q learning, we utilize a neural network to approximate the Q value function. The network receives the state as an input (whether is the frame of the current state or a single value) and outputs the Q values for all possible actions. The biggest output is our next action. We can see that we are not constrained to Fully Connected Neural Networks, but we can use Convolutional, Recurrent and whatever else type of model suits our needs. I think it’s time to use all that stuff in practice and teach the agent play Mountain Car. The goal is to make a car drive up a hill. The car’s engine is not strong enough to climb the hill in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. I will explain more about Deep Q Networks alongside with the code. First we should build our Agent as a Neural Network with 3 Dense layers and we are goint to train it using Adam optimization. class DQNAgent: def __init__(self, state_size, action_size): self.state_size = state_size self.action_size = action_size self.memory = deque(maxlen=2000) self.gamma = 0.95 # discount rate self.epsilon = 1.0 # exploration rate self.epsilon_min = 0.01 self.epsilon_decay = 0.995 self.learning_rate = 0.001 self.model = self._build_model() def _build_model(self): model = Sequential() model.add(Dense(24, input_dim=self.state_size, activation=&#39;relu&#39;)) model.add(Dense(24, activation=&#39;relu&#39;)) model.add(Dense(self.action_size, activation=&#39;linear&#39;)) model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=self.learning_rate)) return model def remember(self, state, action, reward, next_state, done): self.memory.append((state, action, reward, next_state, done)) #get action def act(self, state): #select random action with prob=epsilon else action=maxQ if np.random.rand() &lt;= self.epsilon: return random.randrange(self.action_size) act_values = self.model.predict(state) return np.argmax(act_values[0]) Keypoints: The agent holds a memory buffer with all past experiences. His next action is determined by the maximum output (Q-value) of the network. The loss function is the mean squared error of the predicted Q value and the target Q-value. From the Bellman equation we have that the target is R + g*max(Q). The difference between the target and the predicted values is called Temporal Difference Error (TD Error) Before we train our DQN, we need to address an issue that plays a vital role on how the agent learns to estimate Q Values and this is: Experience Replay Experience replay is a concept where we help the agent to remember and not forget its previous actions by replaying them. Every once in a while, we sample a batch of previous experiences (which are stored in a buffer) and we feed the network. That way the agent relives its past and improve its memory. Another reason for this task is to force the agent to release himself from oscillation, which occurs due to high correlation between some states and resulting in the same actions over and over. def replay(self, batch_size): #sample random transitions minibatch = random.sample(self.memory, batch_size) for state, action, reward, next_state, done in minibatch: target = reward if not done: Q_next=self.model.predict(next_state)[0] target = (reward + self.gamma *np.amax(Q_next)) target_f = self.model.predict(state) target_f[0][action] = target #train network self.model.fit(state, target_f, epochs=1, verbose=0) Finally we get make our agent interact with the environment and train him to predict the Q values for each next action env = gym.make(&#39;MountainCar-v0&#39;) state_size = env.observation_space.shape[0] action_size = env.action_space.n agent = DQNAgent(state_size, action_size) done = False batch_size = 32 for e in range(EPISODES): state = env.reset() state = np.reshape(state, [1, state_size]) for time in range(500): #env.render()# need OpenGL to run action = agent.act(state) next_state, reward, done, _ = env.step(action) reward = reward if not done else -10 next_state = np.reshape(next_state, [1, state_size]) #add to experience memory agent.remember(state, action, reward, next_state, done) state = next_state if done: print(&quot;episode: {}/{}, score: {}, e: {:.2}&quot; .format(e, EPISODES, time, agent.epsilon)) break #experience replay if len(agent.memory) &gt; batch_size: agent.replay(batch_size) As you can see is the exact same process with the Q-table example, with the difference that the next action comes by the DQN prediction and not by the Q-table. As a result, it can be applied to unknown states. That’s the magic of Neural Networks. You just created an agent that learns to drive the car up the hill. Awesome. And what is more awesome is that the exact same code(i mean copy paste) can be used in many more games, from Atari and Super Mario to Doom(!!!) Awesome! Just on more time, I promise. Awesome! In the next episode, we will remain on the Deep Q Learning area and discuss some more advanced techniques such as Double DQN Networks, Dueling DQN and Priotitized Experience replay. See you soon…" />
<meta property="og:description" content="Deep Q Learning The journey to Reinforcement learning continues… It’s time to analyze the infamous Q-learning and see how it became the new standard in the field of AI (with a little help from neural networks). First things first. In the last post , we saw the basic concept behind Reinforcement Learning and we frame the problem using an agent, an environment, a state (S), an action(A) and a reward (R). We talked about how the whole process can be described as a Markov Decision Process and we introduced the terms Policy and Value. Lastly, we had a quick high-level overview of the basic methods out there. Remember that the goal is to find the optimal policy and that policy is a mapping between state and actions. So, we need to find which action to take while we stand in a specific state in order to maximize our expected reward. One way to find the optimal policy is to make use of the value functions (a model-free technique). And here we will get to the new stuff. In fact, there are two value functions that are used today. The state value function V(s) and the action value function Q(s, a) . State value function: Is the expected return achieved when acting from a state according to the policy. Action value function: Is the expected return given the state and the action. What is the difference you may ask? The first value is the value of a particular state. The second one is the value of that state plus the values of all the possible actions from that state. When we have the action value function, the Q value, we can simply choose to perform the action with the highest value from a state. But how do we find the Q value?. What is Q learning? So, we will learn the Q value from trial and error? Exactly. We initialize the Q, we choose an action and perform it, we evaluate it by measuring the reward and we update the Q accordingly. In first, randomness will be a key player but as the agent explores the environment, the algorithm will find the best Q value for each state and action. Can we describe this mathematically? Thank you Richard E. Bellmann. The above equation is known as the Bellman equation and plays a huge part in RL today’s research. But what does it state? The Q value, aka the maximum future reward for a state and action, is the immediate reward plus the maximum future reward for the next state. And if you think about it, it makes perfect sense. Gamma (γ) is a number between [0,1] and its used to discount the reward as the time passes, given the assumption that action in the beginning, are more important than at the end (an assumption that is confirmed by many real-life use cases). As a result, we can update the Q value iteratively. The basic concept to understand here is that the Bellman equation relates states with each other and thus, it relates Action value functions. That helps us iterate over the environment and compute the optimal Values, which in turn give us the optimal Policy. In its simplest form, Q values is a matrix with states as rows and actions as columns. We initialize the Q-matrix randomly, the agent starts to interact with the environment and measures the reward for each action. It then computes the observed Q values and updates the matrix. env = gym.make(&#39;MountainCar-v0&#39;) #initalize Q tabe with zeros Q = np.zeros([env.observation_space.n,env.action_space.n]) for i in range(episodes): s = env.reset() reward = 0 goal_flag = False for j in range(200): # greedy action a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) s_new,r,goal_flag,_ = env.step(a)#state and reward maxQ=np.max(Q[s_new,:]) # Belmann Q[s,a] += lr*(r + g*maxQ - Q[s,a]) reward += r s = s_new if goal_flag == True: break Exploration vs Exploitation The algorithm, as described above, is a greedy algorithm, as it always chooses the action with the best value. But what if some action has a very small probability to produce a very large reward? The agent will never get there. This is fixed by adding random exploration. Every once in a while, the agent will perform a random move, without considering the optimal policy. But because we want the algorithm to converge at some point, we lower the probability to take a random action as the game proceeds. Why going Deep? Q learning is good. No one can deny that. But the fact that it is ineffective in big state spaces remains. Imagine a game with 1000 states and 1000 actions per state. We would need a table of 1 million cells. And that is a very small state space comparing to chess or Go. Also, Q learning can’t be used in unknown states because it can’t infer the Q value of new states from the previous ones. What if we approximate the Q values using some machine learning model. What if we approximate them using neural networks? That simple idea (and execution of course) was the reason behind DeepMind acquisition from Google for 500 million dollars. DeepMind proposed an algorithm named Deep Q Learner and used it to play Atari games with impeccable mastery. Deep Q Learning In deep Q learning, we utilize a neural network to approximate the Q value function. The network receives the state as an input (whether is the frame of the current state or a single value) and outputs the Q values for all possible actions. The biggest output is our next action. We can see that we are not constrained to Fully Connected Neural Networks, but we can use Convolutional, Recurrent and whatever else type of model suits our needs. I think it’s time to use all that stuff in practice and teach the agent play Mountain Car. The goal is to make a car drive up a hill. The car’s engine is not strong enough to climb the hill in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. I will explain more about Deep Q Networks alongside with the code. First we should build our Agent as a Neural Network with 3 Dense layers and we are goint to train it using Adam optimization. class DQNAgent: def __init__(self, state_size, action_size): self.state_size = state_size self.action_size = action_size self.memory = deque(maxlen=2000) self.gamma = 0.95 # discount rate self.epsilon = 1.0 # exploration rate self.epsilon_min = 0.01 self.epsilon_decay = 0.995 self.learning_rate = 0.001 self.model = self._build_model() def _build_model(self): model = Sequential() model.add(Dense(24, input_dim=self.state_size, activation=&#39;relu&#39;)) model.add(Dense(24, activation=&#39;relu&#39;)) model.add(Dense(self.action_size, activation=&#39;linear&#39;)) model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=self.learning_rate)) return model def remember(self, state, action, reward, next_state, done): self.memory.append((state, action, reward, next_state, done)) #get action def act(self, state): #select random action with prob=epsilon else action=maxQ if np.random.rand() &lt;= self.epsilon: return random.randrange(self.action_size) act_values = self.model.predict(state) return np.argmax(act_values[0]) Keypoints: The agent holds a memory buffer with all past experiences. His next action is determined by the maximum output (Q-value) of the network. The loss function is the mean squared error of the predicted Q value and the target Q-value. From the Bellman equation we have that the target is R + g*max(Q). The difference between the target and the predicted values is called Temporal Difference Error (TD Error) Before we train our DQN, we need to address an issue that plays a vital role on how the agent learns to estimate Q Values and this is: Experience Replay Experience replay is a concept where we help the agent to remember and not forget its previous actions by replaying them. Every once in a while, we sample a batch of previous experiences (which are stored in a buffer) and we feed the network. That way the agent relives its past and improve its memory. Another reason for this task is to force the agent to release himself from oscillation, which occurs due to high correlation between some states and resulting in the same actions over and over. def replay(self, batch_size): #sample random transitions minibatch = random.sample(self.memory, batch_size) for state, action, reward, next_state, done in minibatch: target = reward if not done: Q_next=self.model.predict(next_state)[0] target = (reward + self.gamma *np.amax(Q_next)) target_f = self.model.predict(state) target_f[0][action] = target #train network self.model.fit(state, target_f, epochs=1, verbose=0) Finally we get make our agent interact with the environment and train him to predict the Q values for each next action env = gym.make(&#39;MountainCar-v0&#39;) state_size = env.observation_space.shape[0] action_size = env.action_space.n agent = DQNAgent(state_size, action_size) done = False batch_size = 32 for e in range(EPISODES): state = env.reset() state = np.reshape(state, [1, state_size]) for time in range(500): #env.render()# need OpenGL to run action = agent.act(state) next_state, reward, done, _ = env.step(action) reward = reward if not done else -10 next_state = np.reshape(next_state, [1, state_size]) #add to experience memory agent.remember(state, action, reward, next_state, done) state = next_state if done: print(&quot;episode: {}/{}, score: {}, e: {:.2}&quot; .format(e, EPISODES, time, agent.epsilon)) break #experience replay if len(agent.memory) &gt; batch_size: agent.replay(batch_size) As you can see is the exact same process with the Q-table example, with the difference that the next action comes by the DQN prediction and not by the Q-table. As a result, it can be applied to unknown states. That’s the magic of Neural Networks. You just created an agent that learns to drive the car up the hill. Awesome. And what is more awesome is that the exact same code(i mean copy paste) can be used in many more games, from Atari and Super Mario to Doom(!!!) Awesome! Just on more time, I promise. Awesome! In the next episode, we will remain on the Deep Q Learning area and discuss some more advanced techniques such as Double DQN Networks, Dueling DQN and Priotitized Experience replay. See you soon…" />
<link rel="canonical" href="/Deep_Q_Learning/" />
<meta property="og:url" content="/Deep_Q_Learning/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-01T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"/Deep_Q_Learning/","headline":"Deep Q Learning","dateModified":"2018-10-01T00:00:00+03:00","datePublished":"2018-10-01T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"/Deep_Q_Learning/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"Deep Q Learning The journey to Reinforcement learning continues… It’s time to analyze the infamous Q-learning and see how it became the new standard in the field of AI (with a little help from neural networks). First things first. In the last post , we saw the basic concept behind Reinforcement Learning and we frame the problem using an agent, an environment, a state (S), an action(A) and a reward (R). We talked about how the whole process can be described as a Markov Decision Process and we introduced the terms Policy and Value. Lastly, we had a quick high-level overview of the basic methods out there. Remember that the goal is to find the optimal policy and that policy is a mapping between state and actions. So, we need to find which action to take while we stand in a specific state in order to maximize our expected reward. One way to find the optimal policy is to make use of the value functions (a model-free technique). And here we will get to the new stuff. In fact, there are two value functions that are used today. The state value function V(s) and the action value function Q(s, a) . State value function: Is the expected return achieved when acting from a state according to the policy. Action value function: Is the expected return given the state and the action. What is the difference you may ask? The first value is the value of a particular state. The second one is the value of that state plus the values of all the possible actions from that state. When we have the action value function, the Q value, we can simply choose to perform the action with the highest value from a state. But how do we find the Q value?. What is Q learning? So, we will learn the Q value from trial and error? Exactly. We initialize the Q, we choose an action and perform it, we evaluate it by measuring the reward and we update the Q accordingly. In first, randomness will be a key player but as the agent explores the environment, the algorithm will find the best Q value for each state and action. Can we describe this mathematically? Thank you Richard E. Bellmann. The above equation is known as the Bellman equation and plays a huge part in RL today’s research. But what does it state? The Q value, aka the maximum future reward for a state and action, is the immediate reward plus the maximum future reward for the next state. And if you think about it, it makes perfect sense. Gamma (γ) is a number between [0,1] and its used to discount the reward as the time passes, given the assumption that action in the beginning, are more important than at the end (an assumption that is confirmed by many real-life use cases). As a result, we can update the Q value iteratively. The basic concept to understand here is that the Bellman equation relates states with each other and thus, it relates Action value functions. That helps us iterate over the environment and compute the optimal Values, which in turn give us the optimal Policy. In its simplest form, Q values is a matrix with states as rows and actions as columns. We initialize the Q-matrix randomly, the agent starts to interact with the environment and measures the reward for each action. It then computes the observed Q values and updates the matrix. env = gym.make(&#39;MountainCar-v0&#39;) #initalize Q tabe with zeros Q = np.zeros([env.observation_space.n,env.action_space.n]) for i in range(episodes): s = env.reset() reward = 0 goal_flag = False for j in range(200): # greedy action a = np.argmax(Q[s,:] + np.random.randn(1,env.action_space.n)*(1./(i+1))) s_new,r,goal_flag,_ = env.step(a)#state and reward maxQ=np.max(Q[s_new,:]) # Belmann Q[s,a] += lr*(r + g*maxQ - Q[s,a]) reward += r s = s_new if goal_flag == True: break Exploration vs Exploitation The algorithm, as described above, is a greedy algorithm, as it always chooses the action with the best value. But what if some action has a very small probability to produce a very large reward? The agent will never get there. This is fixed by adding random exploration. Every once in a while, the agent will perform a random move, without considering the optimal policy. But because we want the algorithm to converge at some point, we lower the probability to take a random action as the game proceeds. Why going Deep? Q learning is good. No one can deny that. But the fact that it is ineffective in big state spaces remains. Imagine a game with 1000 states and 1000 actions per state. We would need a table of 1 million cells. And that is a very small state space comparing to chess or Go. Also, Q learning can’t be used in unknown states because it can’t infer the Q value of new states from the previous ones. What if we approximate the Q values using some machine learning model. What if we approximate them using neural networks? That simple idea (and execution of course) was the reason behind DeepMind acquisition from Google for 500 million dollars. DeepMind proposed an algorithm named Deep Q Learner and used it to play Atari games with impeccable mastery. Deep Q Learning In deep Q learning, we utilize a neural network to approximate the Q value function. The network receives the state as an input (whether is the frame of the current state or a single value) and outputs the Q values for all possible actions. The biggest output is our next action. We can see that we are not constrained to Fully Connected Neural Networks, but we can use Convolutional, Recurrent and whatever else type of model suits our needs. I think it’s time to use all that stuff in practice and teach the agent play Mountain Car. The goal is to make a car drive up a hill. The car’s engine is not strong enough to climb the hill in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. I will explain more about Deep Q Networks alongside with the code. First we should build our Agent as a Neural Network with 3 Dense layers and we are goint to train it using Adam optimization. class DQNAgent: def __init__(self, state_size, action_size): self.state_size = state_size self.action_size = action_size self.memory = deque(maxlen=2000) self.gamma = 0.95 # discount rate self.epsilon = 1.0 # exploration rate self.epsilon_min = 0.01 self.epsilon_decay = 0.995 self.learning_rate = 0.001 self.model = self._build_model() def _build_model(self): model = Sequential() model.add(Dense(24, input_dim=self.state_size, activation=&#39;relu&#39;)) model.add(Dense(24, activation=&#39;relu&#39;)) model.add(Dense(self.action_size, activation=&#39;linear&#39;)) model.compile(loss=&#39;mse&#39;, optimizer=Adam(lr=self.learning_rate)) return model def remember(self, state, action, reward, next_state, done): self.memory.append((state, action, reward, next_state, done)) #get action def act(self, state): #select random action with prob=epsilon else action=maxQ if np.random.rand() &lt;= self.epsilon: return random.randrange(self.action_size) act_values = self.model.predict(state) return np.argmax(act_values[0]) Keypoints: The agent holds a memory buffer with all past experiences. His next action is determined by the maximum output (Q-value) of the network. The loss function is the mean squared error of the predicted Q value and the target Q-value. From the Bellman equation we have that the target is R + g*max(Q). The difference between the target and the predicted values is called Temporal Difference Error (TD Error) Before we train our DQN, we need to address an issue that plays a vital role on how the agent learns to estimate Q Values and this is: Experience Replay Experience replay is a concept where we help the agent to remember and not forget its previous actions by replaying them. Every once in a while, we sample a batch of previous experiences (which are stored in a buffer) and we feed the network. That way the agent relives its past and improve its memory. Another reason for this task is to force the agent to release himself from oscillation, which occurs due to high correlation between some states and resulting in the same actions over and over. def replay(self, batch_size): #sample random transitions minibatch = random.sample(self.memory, batch_size) for state, action, reward, next_state, done in minibatch: target = reward if not done: Q_next=self.model.predict(next_state)[0] target = (reward + self.gamma *np.amax(Q_next)) target_f = self.model.predict(state) target_f[0][action] = target #train network self.model.fit(state, target_f, epochs=1, verbose=0) Finally we get make our agent interact with the environment and train him to predict the Q values for each next action env = gym.make(&#39;MountainCar-v0&#39;) state_size = env.observation_space.shape[0] action_size = env.action_space.n agent = DQNAgent(state_size, action_size) done = False batch_size = 32 for e in range(EPISODES): state = env.reset() state = np.reshape(state, [1, state_size]) for time in range(500): #env.render()# need OpenGL to run action = agent.act(state) next_state, reward, done, _ = env.step(action) reward = reward if not done else -10 next_state = np.reshape(next_state, [1, state_size]) #add to experience memory agent.remember(state, action, reward, next_state, done) state = next_state if done: print(&quot;episode: {}/{}, score: {}, e: {:.2}&quot; .format(e, EPISODES, time, agent.epsilon)) break #experience replay if len(agent.memory) &gt; batch_size: agent.replay(batch_size) As you can see is the exact same process with the Q-table example, with the difference that the next action comes by the DQN prediction and not by the Q-table. As a result, it can be applied to unknown states. That’s the magic of Neural Networks. You just created an agent that learns to drive the car up the hill. Awesome. And what is more awesome is that the exact same code(i mean copy paste) can be used in many more games, from Atari and Super Mario to Doom(!!!) Awesome! Just on more time, I promise. Awesome! In the next episode, we will remain on the Deep Q Learning area and discuss some more advanced techniques such as Double DQN Networks, Dueling DQN and Priotitized Experience replay. See you soon…","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href=""> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( /assets/img/posts/DQN.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">Deep Q Learning</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Oct 1, 2018</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            12 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="deep-q-learning">Deep Q Learning</h1>

<p>The journey to Reinforcement learning continues… It’s time to analyze the
infamous Q-learning and see how it became the new standard in the field of AI
(with a little help from  neural networks).</p>

<p>First things first. In the <a href="https://sergioskar.github.io/Reinforcement_learning/">last post</a> , we saw the basic concept
behind Reinforcement Learning and we frame the problem using an agent, an
environment, a state (S), an action(A) and a reward (R). We talked about how the
whole process can be described as a Markov Decision Process and we introduced
the terms Policy and Value. Lastly, we had a quick high-level overview of the
basic methods out there.</p>

<p>Remember that the goal is to find the optimal policy and that policy is a
mapping between state and actions. So, we need to find which action to take
while we stand in a specific state in order to maximize our expected reward. One
way to find the optimal policy is to make use of the value functions (a
model-free technique).</p>

<p>And here we will get to the new stuff. In fact, there are two value functions
that are used today. The state value function V(s) and the action value function
Q(s, a) .</p>

<ul>
  <li>
    <p>State value function: Is the expected return achieved when acting from a
state according to the policy.</p>
  </li>
  <li>
    <p>Action value function: Is the expected return given the state and the
action.</p>
  </li>
</ul>

<p>What is the difference you may ask? The first value is the value of a particular
state. The second one is the value of that state plus the values of all the
possible actions from that state.</p>

<p><img src="/assets/img/posts/QValue.jpg" alt="Qvalue" /></p>

<p>When we have the action value function, the Q value, we can simply choose to
perform the action with the highest value from a state. But how do we find the Q
value?.</p>

<h2 id="what-is-q-learning">What is Q learning?</h2>

<p>So, we will learn the Q value from trial and error? Exactly. We initialize the
Q, we choose an action and perform it, we evaluate it by measuring the reward
and we update the Q accordingly. In first, randomness will be a key player but
as the agent explores the environment, the algorithm will find the best Q value
for each state and action. Can we describe this mathematically?</p>

<p><img src="/assets/img/posts/bellman.jpg" alt="bellman" /></p>

<p>Thank you Richard E. Bellmann. The above equation is known as the Bellman
equation and plays a huge part in RL today’s research. But what does it state?</p>

<p>The Q value, aka the maximum future reward for a state and action, is the
immediate reward plus the maximum future reward for the next state. And if you
think about it, it makes perfect sense. Gamma (γ) is a number between [0,1] and
its used to discount the reward as the time passes, given the assumption that
action in the beginning, are more important than at the end (an assumption that
is confirmed by many real-life use cases). As a result, we can <strong>update the Q
value iteratively</strong>.</p>

<p>The basic concept to understand here is that the Bellman equation relates
states with each other and thus, it relates Action value functions. That helps
us iterate over the environment and compute the optimal Values, which in turn
give us the optimal Policy.</p>

<p>In its simplest form, Q values is a matrix with states as rows and actions as
columns. We initialize the Q-matrix randomly, the agent starts to interact with
the environment and measures the reward for each action. It then computes the
observed Q values and updates the matrix.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'MountainCar-v0'</span><span class="p">)</span>
<span class="c1">#initalize Q tabe with zeros
</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">n</span><span class="p">,</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">):</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">goal_flag</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
        <span class="c1"># greedy action
</span>        <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,:]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
        <span class="n">s_new</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">goal_flag</span><span class="p">,</span><span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="c1">#state and reward
</span>        <span class="n">maxQ</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">s_new</span><span class="p">,:])</span>
        <span class="c1"># Belmann
</span>        <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">lr</span><span class="o">*</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">g</span><span class="o">*</span><span class="n">maxQ</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">])</span>

        <span class="n">reward</span> <span class="o">+=</span> <span class="n">r</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">s_new</span>

        <span class="k">if</span> <span class="n">goal_flag</span> <span class="o">==</span> <span class="bp">True</span><span class="p">:</span>
            <span class="k">break</span>
</code></pre></div></div>

<h2 id="exploration-vs-exploitation">Exploration vs Exploitation</h2>

<p>The algorithm, as described above, is a greedy algorithm, as it always chooses the
action with the best value. But what if some action has a very small probability
to produce a very large reward? The agent will never get there. This is fixed by
adding random exploration. Every once in a while, the agent will perform a
random move, without considering the optimal policy. But because we want the
algorithm to converge at some point, we lower the probability to take a random
action as the game proceeds.</p>

<h2 id="why-going-deep">Why going Deep?</h2>

<p>Q learning is good. No one can deny that. But the fact that it is ineffective in
big state spaces remains. Imagine a game with 1000 states and 1000 actions per
state. We would need a table of 1 million cells. And that is a very small state
space comparing to chess or Go. Also, Q learning can’t be used in unknown states
because it can’t infer the Q value of new states from the previous ones.</p>

<p>What if we approximate the Q values using some machine learning model. What if
we approximate them using neural networks? That simple idea (and execution of
course) was the reason behind DeepMind acquisition from Google for 500 million
dollars. DeepMind proposed an algorithm named Deep Q Learner and used it to play
Atari games with impeccable mastery.</p>

<h2 id="deep-q-learning-1">Deep Q Learning</h2>

<p>In deep Q learning, we utilize a neural network to approximate the Q value
function. The network receives the state as an input (whether is the frame of
the current state or a single value) and outputs the Q values for all possible
actions. The biggest output is our next action. We can see that we are not
constrained to Fully Connected Neural Networks, but we can use Convolutional,
Recurrent and whatever else type of model suits our needs.</p>

<p><img src="/assets/img/posts/DQN.jpg" alt="DQN" /></p>

<p>I think it’s time to use all that stuff in practice and teach the agent play
<a href="https://gym.openai.com/envs/MountainCar-v0/">Mountain Car</a>. The goal is to make
a car drive up a hill. The car’s engine is not strong enough to climb the hill
in a single pass. Therefore, the only way to succeed is to drive back and forth
to build up momentum.</p>

<p><img src="/assets/img/posts/Cartpool.jpg" alt="Cartpool" /></p>

<p>I will explain more about Deep Q Networks alongside with the code. First we should
build our Agent as a Neural Network with 3 Dense layers and we are goint to train it
using Adam optimization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.95</span>    <span class="c1"># discount rate
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># exploration rate
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_min</span> <span class="o">=</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon_decay</span> <span class="o">=</span> <span class="mf">0.995</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.001</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
        <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'linear'</span><span class="p">))</span>
        <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mse'</span><span class="p">,</span>
                      <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model</span>

    <span class="k">def</span> <span class="nf">remember</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">))</span>

    <span class="c1">#get action
</span>    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="c1">#select random action with prob=epsilon else action=maxQ
</span>        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">randrange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">)</span>
        <span class="n">act_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">act_values</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  

</code></pre></div></div>
<p>Keypoints:</p>
<ul>
  <li>The agent holds a memory buffer with all past experiences.</li>
  <li>His next action is determined by the maximum output (Q-value) of the network.</li>
  <li>The loss function is the mean squared error of the predicted Q value and the target Q-value.</li>
  <li>From the Bellman equation we have that the target is R + g*max(Q).</li>
  <li>The difference between the target and the predicted values is called Temporal Difference Error (TD Error)</li>
</ul>

<p>Before we train our DQN, we need to address an issue that plays a vital role on how the agent learns to estimate Q Values and this is:</p>

<h2 id="experience-replay">Experience Replay</h2>

<p>Experience replay is a concept where we help the agent to remember and not forget its previous actions by replaying them. Every once in a while, we sample a batch of previous experiences (which are stored in a buffer) and we feed the network. That way the agent relives its past and improve its memory. Another reason for this task is to force the agent to release himself from oscillation, which occurs due to high correlation between some states and resulting in the same actions over and over.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">replay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="c1">#sample random transitions
</span>        <span class="n">minibatch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span> <span class="ow">in</span> <span class="n">minibatch</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">reward</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span><span class="p">:</span>
                <span class="n">Q_next</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">next_state</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">target</span> <span class="o">=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">amax</span><span class="p">(</span><span class="n">Q_next</span><span class="p">))</span>

            <span class="n">target_f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">target_f</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">action</span><span class="p">]</span> <span class="o">=</span> <span class="n">target</span>
            <span class="c1">#train network
</span>            <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">target_f</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally we get make our agent interact with the environment and train him 
to predict the Q values for each next action</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s">'MountainCar-v0'</span><span class="p">)</span>
<span class="n">state_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">action_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">)</span>
<span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPISODES</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>

    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
        <span class="c1">#env.render()# need OpenGL to run
</span>        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">reward</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">done</span> <span class="k">else</span> <span class="o">-</span><span class="mi">10</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">next_state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">state_size</span><span class="p">])</span>
        <span class="c1">#add to experience memory
</span>        <span class="n">agent</span><span class="o">.</span><span class="n">remember</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"episode: {}/{}, score: {}, e: {:.2}"</span>
                  <span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">EPISODES</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">epsilon</span><span class="p">))</span>
            <span class="k">break</span>
    <span class="c1">#experience replay
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">batch_size</span><span class="p">:</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">replay</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
</code></pre></div></div>

<p>As you can see is the exact same process with the Q-table example, with the difference
that the next action comes by the DQN prediction and not by the Q-table. As a result, 
it can be applied to <strong>unknown</strong> states. That’s the magic of Neural Networks.</p>

<p>You just created an agent that learns to drive the car up the hill. Awesome. And what is
more awesome is that the exact same code(i mean copy paste) can be used in many more games, 
from Atari and Super Mario to Doom(!!!)</p>

<p>Awesome!</p>

<p>Just on more time, I promise.</p>

<p>Awesome!</p>

<p>In the next episode, we will remain on the Deep Q Learning area and discuss some more advanced
techniques such as Double DQN Networks, Dueling DQN and Priotitized Experience replay.</p>

<p>See you soon…</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Reinforcement_learning/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            The secrets behind Reinforc...
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Taking_Deep_Q_Networks_a_step_further/">
            Taking Deep Q Networks a st...
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="/lib/jquery/jquery.min.js"></script>
 <script src="/lib/jquery/jquery-migrate.min.js"></script>
 <script src="/lib/popper/popper.min.js"></script>
 <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="/lib/easing/easing.min.js"></script>
 <script src="/lib/counterup/jquery.waypoints.min.js"></script>
 <script src="/lib/counterup/jquery.counterup.js"></script>
 <script src="/lib/lightbox/js/lightbox.min.js"></script>
 <script src="/lib/typed/typed.min.js"></script>
 <script src="/js/scripts.js"></script>
 <script src="/js/contact_form.js"></script>



  </body>

</html>



