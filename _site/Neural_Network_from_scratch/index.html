
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Neural Network from scratch-part 1 | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Neural Network from scratch-part 1" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Neural network library from scratch(part 1) Fully Connected Neural Network Let’s build a neural network from scratch. I mean why not? You may say : Pff… Big deal.. With Python and Numpy it’s just a matter of hours. What if I told you that i will use C++. Nah I’m kidding. I am going to use C. The reason for that is that i want to train my network on GPU and GPUs don’t understand Python, not even C++. My plan is to use OpenCL along with C++ to build a fully functional library to create your own Neural Network and train it. And to spice it up a little , why not implementing a convolutional neural netwok instead of a simple, boring Fully Connected NN. But first things first. Let’s not dive immediately on GPU’s kernel code. First we should build our library skeleton. // First initialize OpenCL OpenCL::initialize_OpenCL(); //Create vectors for input and targes std::vector&lt;std::vector&lt;float&gt; &gt; inputs, targets; std::vector&lt;std::vector&lt;float&gt; &gt; testinputs; std::vector&lt;float&gt; testtargets; //Define our neural network ConvNN m_nn; std::vector&lt;int&gt; netVec; netVec = { 1024,10 }; m_nn.createFullyConnectedNN(netVec, 1, 32); //Train the network m_nn.trainFCNN(inputs, targets, testinputs, testtargets, 50000); //Test accuracy on test data m_nn.trainingAccuracy(testinputs, testtargets, 2000, 1); Ok thats the ordinary process of every machine learning pipeline with the difference that instead of Sklearn or Tensorflow functions, here we have C++. Quite the accomplishment! Right? So far so good. We have the baseline of our software. Now it is time to develop the actual structure of a neural network. The basic entity of any NN is the Node and many nodes stacked together form a layer. There you have it: typedef struct Node { int numberOfWeights; float weights[1200]; float output; float delta; }Node; typedef struct Layer { int numOfNodes; Node nodes[1200]; }Layer; Since this is plain C, we can’t use an std::vector and we need plain C because the above code will be compiled and executed by the actual GPU. But we’re getting there. Please note that a better way than an array with predefined length would be to malloc the necessary space in memory every time, but that is for some other time. We build our basic structures for the Node and the Layer so it is time to program the actual Network, which is simply a stack of layers. h_netVec = newNetVec; //input layer Layer *inputLayer = layer(h_netVec[0], 0); h_layers.push_back(*inputLayer); ///Create the other layers for (unsigned int i = 1; i &lt;h_netVec.size(); i++) { Layer *hidlayer = layer(h_netVec[i], h_netVec[i - 1]); h_layers.push_back(*hidlayer); } There it is. Our simple Neural network written in C++. In fact, it is nothing more than a vector of layers, with each layer being a vector of Nodes. You may think that our job is done here. Haha! We are not even close. We have to train our network with actual data. This is the time where OpenCL is coming into play. Those vectors can not be accesed by the GPU so we have to transform them into another structure called Buffer, a basic element of OpenCL. But the logic is exactly the same as before. d_InputBuffer = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(float)*inpdim*inpdim); tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[0].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf,CL_TRUE,0,sizeof(Node)*h_layers[0].numOfNodes,h_layers[0].nodes); d_layersBuffers.push_back(tempbuf); for (int i = 1; i&lt;h_layers.size(); i++) { tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[i].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf, CL_TRUE,0, sizeof(Node)*h_layers[i].numOfNodes, h_layers[i].nodes); d_layersBuffers.push_back(tempbuf); } Don’t get confused by all those “cl::” , “clqueue” and “context”. Those are OpenCL stuff. The logic remains intangible. Before we dive into the exciting part ,we have to one more thing. We have to define the OpenCL Kernels. The kernels are the acual code that is executed by the GPU. We need 3 kernels in total: One for the forward propagation One for the backward propagation in the output layer One for the backward in the hiddens layer compoutKern = cl::Kernel(OpenCL::clprogram, &quot;compout&quot;); backpropoutKern = cl::Kernel(OpenCL::clprogram, &quot;backpropout&quot;); bakckprophidKern = cl::Kernel(OpenCL::clprogram, &quot;backprophid&quot;); You guessed it. It is GPU’s turn. I am not goint to get into many details about how OpenCL works and how GPU process the data, but there are some things to remember: GPU’s have many many cores and that’s why they are suitable for parallelization We consides that each core runs the code for a single Node of the layer When the layer computations is completed , we procced to the next layer and so on. Keep those in mind we can now understand easily the next snippet: //forward propagation kernel void compout( global Node* nodes,global Node * prevnodes,int softflag) { const int n = get_global_size(0); const int i = get_global_id(0); float t = 0; for ( int j = 0; j &lt; nodes[i].numberOfWeights; j++) t += nodes[i].weights[j] * prevnodes[j].output; t+=0.1;//bias nodes[i].output =sigmoid(t); } And for the backward propagation we have: kernel void backprophid(global Node* nodes,global Node * prevnodes,global Node *nextnodes,int nextnumNodes,float a) { const int n = get_global_size(0); const int i = get_global_id(0); float delta = 0; for (int j = 0; j !=nextnumNodes; j++) delta += nextnodes[j].delta * nextnodes[j].weights[i]; delta *= devsigmoid(nodes[i].output);break; nodes[i].delta = delta; for (int j = 0; j != nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; } kernel void backpropout(global Node* nodes,global Node * prevnodes,global float* targets,float a,int softflag ) { const int n = get_global_size(0); const int i = get_global_id(0); float delta=0; delta = (nodes[i].output-targets[i])*devsigmoid(nodes[i].output); for (int j = 0; j !=nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; nodes[i].delta=delta; } If you feel lost let me remind you the equations for the back propagation algorithm: Now it all makes sense right? Well that’s it. All we have to do is fed our data and run the kernels . I don’t know if you realised it but we are done. We just build our Neura network completely from scratch and train them in GPU. For the full code please visit my github repository: Neural netwok library In the next part we’ll extend the library to include Convolutional Neural Networks. Stay tuned…" />
<meta property="og:description" content="Neural network library from scratch(part 1) Fully Connected Neural Network Let’s build a neural network from scratch. I mean why not? You may say : Pff… Big deal.. With Python and Numpy it’s just a matter of hours. What if I told you that i will use C++. Nah I’m kidding. I am going to use C. The reason for that is that i want to train my network on GPU and GPUs don’t understand Python, not even C++. My plan is to use OpenCL along with C++ to build a fully functional library to create your own Neural Network and train it. And to spice it up a little , why not implementing a convolutional neural netwok instead of a simple, boring Fully Connected NN. But first things first. Let’s not dive immediately on GPU’s kernel code. First we should build our library skeleton. // First initialize OpenCL OpenCL::initialize_OpenCL(); //Create vectors for input and targes std::vector&lt;std::vector&lt;float&gt; &gt; inputs, targets; std::vector&lt;std::vector&lt;float&gt; &gt; testinputs; std::vector&lt;float&gt; testtargets; //Define our neural network ConvNN m_nn; std::vector&lt;int&gt; netVec; netVec = { 1024,10 }; m_nn.createFullyConnectedNN(netVec, 1, 32); //Train the network m_nn.trainFCNN(inputs, targets, testinputs, testtargets, 50000); //Test accuracy on test data m_nn.trainingAccuracy(testinputs, testtargets, 2000, 1); Ok thats the ordinary process of every machine learning pipeline with the difference that instead of Sklearn or Tensorflow functions, here we have C++. Quite the accomplishment! Right? So far so good. We have the baseline of our software. Now it is time to develop the actual structure of a neural network. The basic entity of any NN is the Node and many nodes stacked together form a layer. There you have it: typedef struct Node { int numberOfWeights; float weights[1200]; float output; float delta; }Node; typedef struct Layer { int numOfNodes; Node nodes[1200]; }Layer; Since this is plain C, we can’t use an std::vector and we need plain C because the above code will be compiled and executed by the actual GPU. But we’re getting there. Please note that a better way than an array with predefined length would be to malloc the necessary space in memory every time, but that is for some other time. We build our basic structures for the Node and the Layer so it is time to program the actual Network, which is simply a stack of layers. h_netVec = newNetVec; //input layer Layer *inputLayer = layer(h_netVec[0], 0); h_layers.push_back(*inputLayer); ///Create the other layers for (unsigned int i = 1; i &lt;h_netVec.size(); i++) { Layer *hidlayer = layer(h_netVec[i], h_netVec[i - 1]); h_layers.push_back(*hidlayer); } There it is. Our simple Neural network written in C++. In fact, it is nothing more than a vector of layers, with each layer being a vector of Nodes. You may think that our job is done here. Haha! We are not even close. We have to train our network with actual data. This is the time where OpenCL is coming into play. Those vectors can not be accesed by the GPU so we have to transform them into another structure called Buffer, a basic element of OpenCL. But the logic is exactly the same as before. d_InputBuffer = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(float)*inpdim*inpdim); tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[0].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf,CL_TRUE,0,sizeof(Node)*h_layers[0].numOfNodes,h_layers[0].nodes); d_layersBuffers.push_back(tempbuf); for (int i = 1; i&lt;h_layers.size(); i++) { tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[i].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf, CL_TRUE,0, sizeof(Node)*h_layers[i].numOfNodes, h_layers[i].nodes); d_layersBuffers.push_back(tempbuf); } Don’t get confused by all those “cl::” , “clqueue” and “context”. Those are OpenCL stuff. The logic remains intangible. Before we dive into the exciting part ,we have to one more thing. We have to define the OpenCL Kernels. The kernels are the acual code that is executed by the GPU. We need 3 kernels in total: One for the forward propagation One for the backward propagation in the output layer One for the backward in the hiddens layer compoutKern = cl::Kernel(OpenCL::clprogram, &quot;compout&quot;); backpropoutKern = cl::Kernel(OpenCL::clprogram, &quot;backpropout&quot;); bakckprophidKern = cl::Kernel(OpenCL::clprogram, &quot;backprophid&quot;); You guessed it. It is GPU’s turn. I am not goint to get into many details about how OpenCL works and how GPU process the data, but there are some things to remember: GPU’s have many many cores and that’s why they are suitable for parallelization We consides that each core runs the code for a single Node of the layer When the layer computations is completed , we procced to the next layer and so on. Keep those in mind we can now understand easily the next snippet: //forward propagation kernel void compout( global Node* nodes,global Node * prevnodes,int softflag) { const int n = get_global_size(0); const int i = get_global_id(0); float t = 0; for ( int j = 0; j &lt; nodes[i].numberOfWeights; j++) t += nodes[i].weights[j] * prevnodes[j].output; t+=0.1;//bias nodes[i].output =sigmoid(t); } And for the backward propagation we have: kernel void backprophid(global Node* nodes,global Node * prevnodes,global Node *nextnodes,int nextnumNodes,float a) { const int n = get_global_size(0); const int i = get_global_id(0); float delta = 0; for (int j = 0; j !=nextnumNodes; j++) delta += nextnodes[j].delta * nextnodes[j].weights[i]; delta *= devsigmoid(nodes[i].output);break; nodes[i].delta = delta; for (int j = 0; j != nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; } kernel void backpropout(global Node* nodes,global Node * prevnodes,global float* targets,float a,int softflag ) { const int n = get_global_size(0); const int i = get_global_id(0); float delta=0; delta = (nodes[i].output-targets[i])*devsigmoid(nodes[i].output); for (int j = 0; j !=nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; nodes[i].delta=delta; } If you feel lost let me remind you the equations for the back propagation algorithm: Now it all makes sense right? Well that’s it. All we have to do is fed our data and run the kernels . I don’t know if you realised it but we are done. We just build our Neura network completely from scratch and train them in GPU. For the full code please visit my github repository: Neural netwok library In the next part we’ll extend the library to include Convolutional Neural Networks. Stay tuned…" />
<link rel="canonical" href="//neural_network_from_scratch/" />
<meta property="og:url" content="//neural_network_from_scratch/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-07-19T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"//neural_network_from_scratch/","headline":"Neural Network from scratch-part 1","dateModified":"2018-07-19T00:00:00+03:00","datePublished":"2018-07-19T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"//neural_network_from_scratch/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"Neural network library from scratch(part 1) Fully Connected Neural Network Let’s build a neural network from scratch. I mean why not? You may say : Pff… Big deal.. With Python and Numpy it’s just a matter of hours. What if I told you that i will use C++. Nah I’m kidding. I am going to use C. The reason for that is that i want to train my network on GPU and GPUs don’t understand Python, not even C++. My plan is to use OpenCL along with C++ to build a fully functional library to create your own Neural Network and train it. And to spice it up a little , why not implementing a convolutional neural netwok instead of a simple, boring Fully Connected NN. But first things first. Let’s not dive immediately on GPU’s kernel code. First we should build our library skeleton. // First initialize OpenCL OpenCL::initialize_OpenCL(); //Create vectors for input and targes std::vector&lt;std::vector&lt;float&gt; &gt; inputs, targets; std::vector&lt;std::vector&lt;float&gt; &gt; testinputs; std::vector&lt;float&gt; testtargets; //Define our neural network ConvNN m_nn; std::vector&lt;int&gt; netVec; netVec = { 1024,10 }; m_nn.createFullyConnectedNN(netVec, 1, 32); //Train the network m_nn.trainFCNN(inputs, targets, testinputs, testtargets, 50000); //Test accuracy on test data m_nn.trainingAccuracy(testinputs, testtargets, 2000, 1); Ok thats the ordinary process of every machine learning pipeline with the difference that instead of Sklearn or Tensorflow functions, here we have C++. Quite the accomplishment! Right? So far so good. We have the baseline of our software. Now it is time to develop the actual structure of a neural network. The basic entity of any NN is the Node and many nodes stacked together form a layer. There you have it: typedef struct Node { int numberOfWeights; float weights[1200]; float output; float delta; }Node; typedef struct Layer { int numOfNodes; Node nodes[1200]; }Layer; Since this is plain C, we can’t use an std::vector and we need plain C because the above code will be compiled and executed by the actual GPU. But we’re getting there. Please note that a better way than an array with predefined length would be to malloc the necessary space in memory every time, but that is for some other time. We build our basic structures for the Node and the Layer so it is time to program the actual Network, which is simply a stack of layers. h_netVec = newNetVec; //input layer Layer *inputLayer = layer(h_netVec[0], 0); h_layers.push_back(*inputLayer); ///Create the other layers for (unsigned int i = 1; i &lt;h_netVec.size(); i++) { Layer *hidlayer = layer(h_netVec[i], h_netVec[i - 1]); h_layers.push_back(*hidlayer); } There it is. Our simple Neural network written in C++. In fact, it is nothing more than a vector of layers, with each layer being a vector of Nodes. You may think that our job is done here. Haha! We are not even close. We have to train our network with actual data. This is the time where OpenCL is coming into play. Those vectors can not be accesed by the GPU so we have to transform them into another structure called Buffer, a basic element of OpenCL. But the logic is exactly the same as before. d_InputBuffer = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(float)*inpdim*inpdim); tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[0].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf,CL_TRUE,0,sizeof(Node)*h_layers[0].numOfNodes,h_layers[0].nodes); d_layersBuffers.push_back(tempbuf); for (int i = 1; i&lt;h_layers.size(); i++) { tempbuf = cl::Buffer(OpenCL::clcontext, CL_MEM_READ_WRITE, sizeof(Node)*h_layers[i].numOfNodes); (OpenCL::clqueue).enqueueWriteBuffer(tempbuf, CL_TRUE,0, sizeof(Node)*h_layers[i].numOfNodes, h_layers[i].nodes); d_layersBuffers.push_back(tempbuf); } Don’t get confused by all those “cl::” , “clqueue” and “context”. Those are OpenCL stuff. The logic remains intangible. Before we dive into the exciting part ,we have to one more thing. We have to define the OpenCL Kernels. The kernels are the acual code that is executed by the GPU. We need 3 kernels in total: One for the forward propagation One for the backward propagation in the output layer One for the backward in the hiddens layer compoutKern = cl::Kernel(OpenCL::clprogram, &quot;compout&quot;); backpropoutKern = cl::Kernel(OpenCL::clprogram, &quot;backpropout&quot;); bakckprophidKern = cl::Kernel(OpenCL::clprogram, &quot;backprophid&quot;); You guessed it. It is GPU’s turn. I am not goint to get into many details about how OpenCL works and how GPU process the data, but there are some things to remember: GPU’s have many many cores and that’s why they are suitable for parallelization We consides that each core runs the code for a single Node of the layer When the layer computations is completed , we procced to the next layer and so on. Keep those in mind we can now understand easily the next snippet: //forward propagation kernel void compout( global Node* nodes,global Node * prevnodes,int softflag) { const int n = get_global_size(0); const int i = get_global_id(0); float t = 0; for ( int j = 0; j &lt; nodes[i].numberOfWeights; j++) t += nodes[i].weights[j] * prevnodes[j].output; t+=0.1;//bias nodes[i].output =sigmoid(t); } And for the backward propagation we have: kernel void backprophid(global Node* nodes,global Node * prevnodes,global Node *nextnodes,int nextnumNodes,float a) { const int n = get_global_size(0); const int i = get_global_id(0); float delta = 0; for (int j = 0; j !=nextnumNodes; j++) delta += nextnodes[j].delta * nextnodes[j].weights[i]; delta *= devsigmoid(nodes[i].output);break; nodes[i].delta = delta; for (int j = 0; j != nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; } kernel void backpropout(global Node* nodes,global Node * prevnodes,global float* targets,float a,int softflag ) { const int n = get_global_size(0); const int i = get_global_id(0); float delta=0; delta = (nodes[i].output-targets[i])*devsigmoid(nodes[i].output); for (int j = 0; j !=nodes[i].numberOfWeights; j++) nodes[i].weights[j] -= a*delta*prevnodes[j].output; nodes[i].delta=delta; } If you feel lost let me remind you the equations for the back propagation algorithm: Now it all makes sense right? Well that’s it. All we have to do is fed our data and run the kernels . I don’t know if you realised it but we are done. We just build our Neura network completely from scratch and train them in GPU. For the full code please visit my github repository: Neural netwok library In the next part we’ll extend the library to include Convolutional Neural Networks. Stay tuned…","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href="/"> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( //assets/img/posts/NN.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">Neural Network from scratch-part 1</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Jul 19, 2018</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            9 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="neural-network-library-from-scratchpart-1">Neural network library from scratch(part 1)</h1>

<h2 id="fully-connected-neural-network">Fully Connected Neural Network</h2>

<p>Let’s build a neural network from scratch. I mean why not?
You may say : Pff… Big deal.. With Python and Numpy it’s just a matter of hours. What if I told you that i will use C++. Nah I’m kidding. I am going to use C.</p>

<p>The reason for that is that i want to train my network on GPU and GPUs don’t understand Python, not even C++. My plan is to use OpenCL along with C++ to build a fully functional library to create your own Neural Network and train it. And to spice it up a little , why not implementing a convolutional neural netwok instead of a simple, boring Fully Connected NN. But first things first.</p>

<p>Let’s not dive immediately on GPU’s kernel code. First we should build our library skeleton.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// First initialize OpenCL</span>
<span class="n">OpenCL</span><span class="o">::</span><span class="n">initialize_OpenCL</span><span class="p">();</span>

<span class="c1">//Create vectors for input and targes</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="o">&gt;</span> <span class="n">testinputs</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span> <span class="n">testtargets</span><span class="p">;</span>

<span class="c1">//Define our neural network</span>
<span class="n">ConvNN</span> <span class="n">m_nn</span><span class="p">;</span>
<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span> <span class="n">netVec</span><span class="p">;</span> 
<span class="n">netVec</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">1024</span><span class="p">,</span><span class="mi">10</span> <span class="p">};</span>
<span class="n">m_nn</span><span class="p">.</span><span class="n">createFullyConnectedNN</span><span class="p">(</span><span class="n">netVec</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">);</span>

<span class="c1">//Train the network</span>
 <span class="n">m_nn</span><span class="p">.</span><span class="n">trainFCNN</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">testinputs</span><span class="p">,</span> <span class="n">testtargets</span><span class="p">,</span> <span class="mi">50000</span><span class="p">);</span>

<span class="c1">//Test accuracy on test data</span>
<span class="n">m_nn</span><span class="p">.</span><span class="n">trainingAccuracy</span><span class="p">(</span><span class="n">testinputs</span><span class="p">,</span> <span class="n">testtargets</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>

</code></pre></div></div>

<p>Ok thats the ordinary process of every machine learning pipeline with the difference that instead of Sklearn or Tensorflow functions, here we have C++. Quite the accomplishment! Right?</p>

<p>So far so good. We have the baseline of our software. Now it is time to develop the actual structure of a neural network. The basic entity of any NN is the Node and many nodes stacked together form a layer. There you have it:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">typedef</span> <span class="k">struct</span> <span class="n">Node</span> <span class="p">{</span>

    <span class="kt">int</span> <span class="n">numberOfWeights</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">weights</span><span class="p">[</span><span class="mi">1200</span><span class="p">];</span>
	<span class="kt">float</span> <span class="n">output</span><span class="p">;</span>
	<span class="kt">float</span> <span class="n">delta</span><span class="p">;</span>

<span class="p">}</span><span class="n">Node</span><span class="p">;</span>

<span class="k">typedef</span> <span class="k">struct</span> <span class="n">Layer</span> <span class="p">{</span>

	<span class="kt">int</span> <span class="n">numOfNodes</span><span class="p">;</span>
	<span class="n">Node</span> <span class="n">nodes</span><span class="p">[</span><span class="mi">1200</span><span class="p">];</span>

<span class="p">}</span><span class="n">Layer</span><span class="p">;</span>
</code></pre></div></div>

<p>Since this is plain C, we can’t use an std::vector and we need plain C because the above code will be compiled and executed by the actual GPU. But we’re getting there. Please note that a better way than an array with predefined length would be to malloc the necessary space in memory every time, but that is for some other time.</p>

<p>We build our basic structures for the Node and the Layer so it is time to program the actual Network, which is simply a stack of layers.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">h_netVec</span> <span class="o">=</span> <span class="n">newNetVec</span><span class="p">;</span>

<span class="c1">//input layer</span>
<span class="n">Layer</span> <span class="o">*</span><span class="n">inputLayer</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_netVec</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">0</span><span class="p">);</span>
<span class="n">h_layers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="o">*</span><span class="n">inputLayer</span><span class="p">);</span>

<span class="c1">///Create the other layers</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span><span class="n">h_netVec</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span>
<span class="p">{</span>
	<span class="n">Layer</span> <span class="o">*</span><span class="n">hidlayer</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">h_netVec</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">h_netVec</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]);</span>
	<span class="n">h_layers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="o">*</span><span class="n">hidlayer</span><span class="p">);</span>

<span class="p">}</span>

</code></pre></div></div>

<p>There it is. Our simple Neural network written in C++. In fact, it is nothing more than a vector of layers, with each layer being a vector of Nodes. You may think that our job is done here. Haha! We are not even close. We have to train our network with actual data. This is the time where OpenCL is coming into play.</p>

<p>Those vectors can not be accesed by the GPU so we have to transform them into another structure called Buffer, a basic element of OpenCL. But the logic is exactly the same as before.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">d_InputBuffer</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clcontext</span><span class="p">,</span> <span class="n">CL_MEM_READ_WRITE</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">)</span><span class="o">*</span><span class="n">inpdim</span><span class="o">*</span><span class="n">inpdim</span><span class="p">);</span>


<span class="n">tempbuf</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clcontext</span><span class="p">,</span> <span class="n">CL_MEM_READ_WRITE</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span><span class="o">*</span><span class="n">h_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">numOfNodes</span><span class="p">);</span>
<span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clqueue</span><span class="p">).</span><span class="n">enqueueWriteBuffer</span><span class="p">(</span><span class="n">tempbuf</span><span class="p">,</span><span class="n">CL_TRUE</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span><span class="o">*</span><span class="n">h_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">numOfNodes</span><span class="p">,</span><span class="n">h_layers</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">nodes</span><span class="p">);</span>
<span class="n">d_layersBuffers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">tempbuf</span><span class="p">);</span>

<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">h_layers</span><span class="p">.</span><span class="n">size</span><span class="p">();</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
	<span class="n">tempbuf</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Buffer</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clcontext</span><span class="p">,</span> <span class="n">CL_MEM_READ_WRITE</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span><span class="o">*</span><span class="n">h_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numOfNodes</span><span class="p">);</span>
	<span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clqueue</span><span class="p">).</span><span class="n">enqueueWriteBuffer</span><span class="p">(</span><span class="n">tempbuf</span><span class="p">,</span> <span class="n">CL_TRUE</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span> <span class="k">sizeof</span><span class="p">(</span><span class="n">Node</span><span class="p">)</span><span class="o">*</span><span class="n">h_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numOfNodes</span><span class="p">,</span> <span class="n">h_layers</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">nodes</span><span class="p">);</span>
	<span class="n">d_layersBuffers</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">tempbuf</span><span class="p">);</span>

<span class="p">}</span>

</code></pre></div></div>

<p>Don’t get confused by all those “cl::” , “clqueue” and “context”. Those are OpenCL stuff. The logic remains intangible.</p>

<p>Before we dive into the exciting part ,we have to one more thing. We have to define the OpenCL Kernels. The kernels are the acual code that is executed by the GPU.
We need 3 kernels in total:</p>
<ul>
  <li>One for the forward propagation</li>
  <li>One for the backward propagation in the output layer</li>
  <li>One for the backward in the hiddens layer</li>
</ul>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">compoutKern</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Kernel</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clprogram</span><span class="p">,</span> <span class="s">"compout"</span><span class="p">);</span>
<span class="n">backpropoutKern</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Kernel</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clprogram</span><span class="p">,</span> <span class="s">"backpropout"</span><span class="p">);</span>
<span class="n">bakckprophidKern</span> <span class="o">=</span> <span class="n">cl</span><span class="o">::</span><span class="n">Kernel</span><span class="p">(</span><span class="n">OpenCL</span><span class="o">::</span><span class="n">clprogram</span><span class="p">,</span> <span class="s">"backprophid"</span><span class="p">);</span>

</code></pre></div></div>

<p>You guessed it. It is GPU’s turn. I am not goint to get into many details about how OpenCL works and how GPU process the data, but there are some things to remember:</p>

<ol>
  <li>GPU’s have many many cores and that’s why they are suitable for parallelization</li>
  <li>We consides that each core runs the code for a single Node of the layer</li>
  <li>When the layer computations is completed , we procced to the next layer and so on.</li>
</ol>

<p>Keep those in mind we can now understand easily the next snippet:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">//forward propagation</span>
<span class="n">kernel</span> <span class="kt">void</span> <span class="nf">compout</span><span class="p">(</span>  <span class="n">global</span> <span class="n">Node</span><span class="o">*</span>  <span class="n">nodes</span><span class="p">,</span><span class="n">global</span> <span class="n">Node</span> <span class="o">*</span> <span class="n">prevnodes</span><span class="p">,</span><span class="kt">int</span> <span class="n">softflag</span><span class="p">)</span>
<span class="p">{</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">get_global_size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
    <span class="k">const</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

    <span class="kt">float</span> <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span> <span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numberOfWeights</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
       <span class="n">t</span> <span class="o">+=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">prevnodes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">output</span><span class="p">;</span>

<span class="n">t</span><span class="o">+=</span><span class="mi">0</span><span class="p">.</span><span class="mi">1</span><span class="p">;</span><span class="c1">//bias</span>

<span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">output</span> <span class="o">=</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">t</span><span class="p">);</span>	

<span class="p">}</span>

</code></pre></div></div>
<p>And for the backward propagation we have:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">kernel</span> <span class="kt">void</span> <span class="nf">backprophid</span><span class="p">(</span><span class="n">global</span> <span class="n">Node</span><span class="o">*</span>  <span class="n">nodes</span><span class="p">,</span><span class="n">global</span> <span class="n">Node</span> <span class="o">*</span> <span class="n">prevnodes</span><span class="p">,</span><span class="n">global</span> <span class="n">Node</span> <span class="o">*</span><span class="n">nextnodes</span><span class="p">,</span><span class="kt">int</span> <span class="n">nextnumNodes</span><span class="p">,</span><span class="kt">float</span> <span class="n">a</span><span class="p">)</span>
<span class="p">{</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">get_global_size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>



<span class="kt">float</span> <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">!=</span><span class="n">nextnumNodes</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
	<span class="n">delta</span> <span class="o">+=</span> <span class="n">nextnodes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">delta</span> <span class="o">*</span> <span class="n">nextnodes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>

<span class="n">delta</span> <span class="o">*=</span> <span class="n">devsigmoid</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">output</span><span class="p">);</span><span class="k">break</span><span class="p">;</span>
<span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span><span class="p">;</span>
   
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">!=</span> <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numberOfWeights</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
        <span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">a</span><span class="o">*</span><span class="n">delta</span><span class="o">*</span><span class="n">prevnodes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">output</span><span class="p">;</span>

<span class="p">}</span>


<span class="n">kernel</span> <span class="kt">void</span> <span class="nf">backpropout</span><span class="p">(</span><span class="n">global</span> <span class="n">Node</span><span class="o">*</span>  <span class="n">nodes</span><span class="p">,</span><span class="n">global</span> <span class="n">Node</span> <span class="o">*</span> <span class="n">prevnodes</span><span class="p">,</span><span class="n">global</span> <span class="kt">float</span><span class="o">*</span> <span class="n">targets</span><span class="p">,</span><span class="kt">float</span> <span class="n">a</span><span class="p">,</span><span class="kt">int</span> <span class="n">softflag</span> <span class="p">)</span>
<span class="p">{</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">n</span> <span class="o">=</span> <span class="n">get_global_size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="k">const</span> <span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="n">get_global_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>

<span class="kt">float</span> <span class="n">delta</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span>

<span class="n">delta</span> <span class="o">=</span> <span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">output</span><span class="o">-</span><span class="n">targets</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">*</span><span class="n">devsigmoid</span><span class="p">(</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">output</span><span class="p">);</span>
		
<span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">j</span> <span class="o">!=</span><span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">numberOfWeights</span><span class="p">;</span> <span class="n">j</span><span class="o">++</span><span class="p">)</span>
	<span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">weights</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-=</span> <span class="n">a</span><span class="o">*</span><span class="n">delta</span><span class="o">*</span><span class="n">prevnodes</span><span class="p">[</span><span class="n">j</span><span class="p">].</span><span class="n">output</span><span class="p">;</span>

<span class="n">nodes</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">delta</span><span class="o">=</span><span class="n">delta</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>

<p>If you feel lost let me remind you the equations for the back propagation algorithm:</p>

<p><img src="//assets/img/posts/bpa_equat.jpg" alt="Equations" /></p>

<p>Now it all makes sense right?</p>

<p>Well that’s it. All we have to do is fed our data and run the kernels . I don’t know if you realised it but we are done. We just build our Neura network completely from scratch and train them in GPU.</p>

<p>For the full code please visit my github repository: <a href="https://github.com/SergiosKar/Convolutional-Neural-Network">Neural netwok library</a></p>

<p>In the next part we’ll extend the library to include Convolutional Neural Networks. Stay tuned…</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Document_clustering/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            Document clustering
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Neural_Network_from_scratch_part2/">
            Neural Network from scratch...
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="//lib/jquery/jquery.min.js"></script>
 <script src="//lib/jquery/jquery-migrate.min.js"></script>
 <script src="//lib/popper/popper.min.js"></script>
 <script src="//lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="//lib/easing/easing.min.js"></script>
 <script src="//lib/counterup/jquery.waypoints.min.js"></script>
 <script src="//lib/counterup/jquery.counterup.js"></script>
 <script src="//lib/lightbox/js/lightbox.min.js"></script>
 <script src="//lib/typed/typed.min.js"></script>
 <script src="//js/scripts.js"></script>
 <script src="//js/contact_form.js"></script>



  </body>

</html>



