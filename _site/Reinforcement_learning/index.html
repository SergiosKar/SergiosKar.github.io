
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>The secrets behind Reinforcement Learning | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="The secrets behind Reinforcement Learning" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The secrets behind Reinforcement Learning Bots that play Dota2, AI that beat the best Go players in the world, computers that excel at Doom. What’s going on? Is there a reason why the AI community has been so busy playing games? Let me put it that way. If you want a robot to learn how to walk what do you do? You build one, program it and release it on the streets of New York? Of course not. You build a simulation, a game, and you use that virtual space to teach it how to move around it. Zero cost, zero risks. That’s why games are so useful in research areas. But how do you teach it to walk? The answer is the topic of today’s article and is probably the most exciting field of Machine learning at the time: You probably knew that there are two types of machine learning. Supervised and unsupervised. Well, there is a third one, called Reinforcement Learning. RL is arguably the most difficult area of ML to understand cause there are so many, many things going on at the same time. I’ll try to simplify as much as I can because it is a really astonishing area and you should definitely know about it. But let me warn you. It involves complex thinking and 100% focus to grasp it. And some math. So, take a deep breath and let’s dive in: Markov Decision Processes Reinforcement learning is a trial and error process where an AI (agent) performs a number of actions in an environment. Each unique moment the agent has a state and acts from this given state to a new one. This particular action may on may not has a reward. Therefore, we can say that each learning epoch (or an episode) can be represented as a sequence of states, actions, and rewards. Each state depends only on the previous states and actions and as the environment is inherently stochastic (we don’t know the state that comes next), this process satisfies the Markov property. The Markov property says that the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. The whole process is referred to as a Markov Decision Process. Let’s see a real example using Super Mario. In this case: The agent is, of course, the beloved Mario The state is the current situation (let’s say the frame of our screen) The actions are: left, right movement and jump The environment is the virtual world of each level; And the reward is whether Mario is alive or dead. Ok, we have properly defined the problem. What’s next? We need a solution. But first, we need a way to evaluate how good the solution is? What I am saying is that the reward on each episode it’s not enough. Imagine a Mario game where the Mario is controlled by an agent. He is receiving constantly positive rewards through the whole level by just before the final flag, he is killed by one Hammer Bro (I hate those guys). You see that each individual reward is not enough for us to win the game. We need a reward that captures the whole level. This is where the term of the discounted cumulative expected reward comes into play. It is nothing more than the sum of all rewards discounted by a factor gamma, where gamma belongs to [0,1). The discount is essential because the rewards tend to be much more significant in the beginning than in the end. And it makes perfect sense. The next step is to solve the problem. To do that we define that the goal of the learning task is: The agent needs to learn which action to perform from a given state that maximized the cumulative reward over time. Or to learn the Policy π: S-&gt;A. The policy is just a mapping between a state and an action. To sum all the above, we use the following equation: , where V (Value) is the expected long-term reward achieved by a policy (π) from a state (s). You still with me? If you are, let’s pause for 5 seconds cause this was a bit overwhelming: 1…2…3…4…5 Now that we regain our mental clarity, let recap. We have the definition of the problem as a Markov Decision Process and we have our goal to learn the best Policy or the best Value. How we proceed? We need an algorithm (Thank you, Sherlock…) Well, there is an abundance of developed RL algorithms over the years. Each algorithm focuses on a different thing, whether it is to maximize the value or the policy or both. Whether to use a model(e.g a neural network) to simulate the environment or not. Whether it will capture the reward on each step or the end. As you guessed, it is not very easy to categorize all those algorithms in classes, but that’s what I am about to do. As you can see we can classify RL algorithms in two big categories: Model-based and Model-free: Model-based These algorithms aim to learn how the environment works (its dynamics) from its observations and then plan a solution using that model. When they have a model, they use some planning method to find the best policy. They known to be data efficient, but they fail when the state space is too large. Try to build a model-based algorithm to play Go. Not gonna happen. Dynamic programming methods are an example of model-based methods, as they require the complete knowledge of the environment, such as transition probabilities and rewards. Model-free Model-free algorithms do not require to learn the environment and store all the combination of states and actions. The can be divided into two categories, based on the ultimate goal of the training. Policy-based methods try to find the optimal policy, whether it’s stochastic or deterministic. Algorithms like policy gradients and REINFORCE belong in this category. Their advantages are better convergence and effectiveness on high dimensional or continuous action spaces. Policy-based methods are essentially an optimization problem, where we find the maximum of a policy function. That’s why we also use algorithms like evolution strategies and hill climbing. Value-based methods, on the other hand, try to find the optimal value. A big part of this category is a family of algorithms called Q-learning, which learn to optimize the Q-Value. I plan to analyze Q-learning thoroughly on a next article because it is an essential aspect of Reinforcement learning. Other algorithms involve SARSA and value iteration. At the intersection of policy and value-based method, we find the Actor-Critic methods, where the goal is to optimize both the policy and the value function. And now to the cool part. In the past few years, there is a new kid in town. And it was inevitable to affect and enhance all the existing methods to solve Reinforcement Learning. I am sure you guessed it. Deep Learning. And thus, we have a new term to represent all those new research ideas. Deep Reinforcement Learning Deep neural networks have been used to model the dynamics of the environment(mode-based), to enhance policy searches (policy-based) and to approximate the Value function (value-based). Research on the last one (which is my favorite) has produced a model called Deep Q Network, which is responsible ,along with its many improvements, for some of the most astonishing breakthroughs around the area (take Atari for example). And to excite you, even more, we don’t just use simple Neural Networks but Convolutional, Recurrent and many else as well. Ok, I think that’s enough for the first contact with Reinforcement Learning. I just wanted to give you the basis behind the whole idea and present you an overview of all the important techniques implemented over the years. But also, to give you a hint of what’s next for the field. Reinforcement learning has applications both in industry and in research. To name a few it has been used for: Robotics control, Optimizing chemical reactions, Recommendation systems, Advertising, Product design, Supply chain optimization, Stock trading. I could go on forever. Its probably the most exciting area of AI right now and in my opinion, it has all the rights to be. Until my next article, stay tuned. For the next article in the Reinforcement Learning Journey, click here." />
<meta property="og:description" content="The secrets behind Reinforcement Learning Bots that play Dota2, AI that beat the best Go players in the world, computers that excel at Doom. What’s going on? Is there a reason why the AI community has been so busy playing games? Let me put it that way. If you want a robot to learn how to walk what do you do? You build one, program it and release it on the streets of New York? Of course not. You build a simulation, a game, and you use that virtual space to teach it how to move around it. Zero cost, zero risks. That’s why games are so useful in research areas. But how do you teach it to walk? The answer is the topic of today’s article and is probably the most exciting field of Machine learning at the time: You probably knew that there are two types of machine learning. Supervised and unsupervised. Well, there is a third one, called Reinforcement Learning. RL is arguably the most difficult area of ML to understand cause there are so many, many things going on at the same time. I’ll try to simplify as much as I can because it is a really astonishing area and you should definitely know about it. But let me warn you. It involves complex thinking and 100% focus to grasp it. And some math. So, take a deep breath and let’s dive in: Markov Decision Processes Reinforcement learning is a trial and error process where an AI (agent) performs a number of actions in an environment. Each unique moment the agent has a state and acts from this given state to a new one. This particular action may on may not has a reward. Therefore, we can say that each learning epoch (or an episode) can be represented as a sequence of states, actions, and rewards. Each state depends only on the previous states and actions and as the environment is inherently stochastic (we don’t know the state that comes next), this process satisfies the Markov property. The Markov property says that the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. The whole process is referred to as a Markov Decision Process. Let’s see a real example using Super Mario. In this case: The agent is, of course, the beloved Mario The state is the current situation (let’s say the frame of our screen) The actions are: left, right movement and jump The environment is the virtual world of each level; And the reward is whether Mario is alive or dead. Ok, we have properly defined the problem. What’s next? We need a solution. But first, we need a way to evaluate how good the solution is? What I am saying is that the reward on each episode it’s not enough. Imagine a Mario game where the Mario is controlled by an agent. He is receiving constantly positive rewards through the whole level by just before the final flag, he is killed by one Hammer Bro (I hate those guys). You see that each individual reward is not enough for us to win the game. We need a reward that captures the whole level. This is where the term of the discounted cumulative expected reward comes into play. It is nothing more than the sum of all rewards discounted by a factor gamma, where gamma belongs to [0,1). The discount is essential because the rewards tend to be much more significant in the beginning than in the end. And it makes perfect sense. The next step is to solve the problem. To do that we define that the goal of the learning task is: The agent needs to learn which action to perform from a given state that maximized the cumulative reward over time. Or to learn the Policy π: S-&gt;A. The policy is just a mapping between a state and an action. To sum all the above, we use the following equation: , where V (Value) is the expected long-term reward achieved by a policy (π) from a state (s). You still with me? If you are, let’s pause for 5 seconds cause this was a bit overwhelming: 1…2…3…4…5 Now that we regain our mental clarity, let recap. We have the definition of the problem as a Markov Decision Process and we have our goal to learn the best Policy or the best Value. How we proceed? We need an algorithm (Thank you, Sherlock…) Well, there is an abundance of developed RL algorithms over the years. Each algorithm focuses on a different thing, whether it is to maximize the value or the policy or both. Whether to use a model(e.g a neural network) to simulate the environment or not. Whether it will capture the reward on each step or the end. As you guessed, it is not very easy to categorize all those algorithms in classes, but that’s what I am about to do. As you can see we can classify RL algorithms in two big categories: Model-based and Model-free: Model-based These algorithms aim to learn how the environment works (its dynamics) from its observations and then plan a solution using that model. When they have a model, they use some planning method to find the best policy. They known to be data efficient, but they fail when the state space is too large. Try to build a model-based algorithm to play Go. Not gonna happen. Dynamic programming methods are an example of model-based methods, as they require the complete knowledge of the environment, such as transition probabilities and rewards. Model-free Model-free algorithms do not require to learn the environment and store all the combination of states and actions. The can be divided into two categories, based on the ultimate goal of the training. Policy-based methods try to find the optimal policy, whether it’s stochastic or deterministic. Algorithms like policy gradients and REINFORCE belong in this category. Their advantages are better convergence and effectiveness on high dimensional or continuous action spaces. Policy-based methods are essentially an optimization problem, where we find the maximum of a policy function. That’s why we also use algorithms like evolution strategies and hill climbing. Value-based methods, on the other hand, try to find the optimal value. A big part of this category is a family of algorithms called Q-learning, which learn to optimize the Q-Value. I plan to analyze Q-learning thoroughly on a next article because it is an essential aspect of Reinforcement learning. Other algorithms involve SARSA and value iteration. At the intersection of policy and value-based method, we find the Actor-Critic methods, where the goal is to optimize both the policy and the value function. And now to the cool part. In the past few years, there is a new kid in town. And it was inevitable to affect and enhance all the existing methods to solve Reinforcement Learning. I am sure you guessed it. Deep Learning. And thus, we have a new term to represent all those new research ideas. Deep Reinforcement Learning Deep neural networks have been used to model the dynamics of the environment(mode-based), to enhance policy searches (policy-based) and to approximate the Value function (value-based). Research on the last one (which is my favorite) has produced a model called Deep Q Network, which is responsible ,along with its many improvements, for some of the most astonishing breakthroughs around the area (take Atari for example). And to excite you, even more, we don’t just use simple Neural Networks but Convolutional, Recurrent and many else as well. Ok, I think that’s enough for the first contact with Reinforcement Learning. I just wanted to give you the basis behind the whole idea and present you an overview of all the important techniques implemented over the years. But also, to give you a hint of what’s next for the field. Reinforcement learning has applications both in industry and in research. To name a few it has been used for: Robotics control, Optimizing chemical reactions, Recommendation systems, Advertising, Product design, Supply chain optimization, Stock trading. I could go on forever. Its probably the most exciting area of AI right now and in my opinion, it has all the rights to be. Until my next article, stay tuned. For the next article in the Reinforcement Learning Journey, click here." />
<link rel="canonical" href="/Reinforcement_learning/" />
<meta property="og:url" content="/Reinforcement_learning/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-09-23T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"/Reinforcement_learning/","headline":"The secrets behind Reinforcement Learning","dateModified":"2018-09-23T00:00:00+03:00","datePublished":"2018-09-23T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"/Reinforcement_learning/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"The secrets behind Reinforcement Learning Bots that play Dota2, AI that beat the best Go players in the world, computers that excel at Doom. What’s going on? Is there a reason why the AI community has been so busy playing games? Let me put it that way. If you want a robot to learn how to walk what do you do? You build one, program it and release it on the streets of New York? Of course not. You build a simulation, a game, and you use that virtual space to teach it how to move around it. Zero cost, zero risks. That’s why games are so useful in research areas. But how do you teach it to walk? The answer is the topic of today’s article and is probably the most exciting field of Machine learning at the time: You probably knew that there are two types of machine learning. Supervised and unsupervised. Well, there is a third one, called Reinforcement Learning. RL is arguably the most difficult area of ML to understand cause there are so many, many things going on at the same time. I’ll try to simplify as much as I can because it is a really astonishing area and you should definitely know about it. But let me warn you. It involves complex thinking and 100% focus to grasp it. And some math. So, take a deep breath and let’s dive in: Markov Decision Processes Reinforcement learning is a trial and error process where an AI (agent) performs a number of actions in an environment. Each unique moment the agent has a state and acts from this given state to a new one. This particular action may on may not has a reward. Therefore, we can say that each learning epoch (or an episode) can be represented as a sequence of states, actions, and rewards. Each state depends only on the previous states and actions and as the environment is inherently stochastic (we don’t know the state that comes next), this process satisfies the Markov property. The Markov property says that the conditional probability distribution of future states of the process depends only upon the present state, not on the sequence of events that preceded it. The whole process is referred to as a Markov Decision Process. Let’s see a real example using Super Mario. In this case: The agent is, of course, the beloved Mario The state is the current situation (let’s say the frame of our screen) The actions are: left, right movement and jump The environment is the virtual world of each level; And the reward is whether Mario is alive or dead. Ok, we have properly defined the problem. What’s next? We need a solution. But first, we need a way to evaluate how good the solution is? What I am saying is that the reward on each episode it’s not enough. Imagine a Mario game where the Mario is controlled by an agent. He is receiving constantly positive rewards through the whole level by just before the final flag, he is killed by one Hammer Bro (I hate those guys). You see that each individual reward is not enough for us to win the game. We need a reward that captures the whole level. This is where the term of the discounted cumulative expected reward comes into play. It is nothing more than the sum of all rewards discounted by a factor gamma, where gamma belongs to [0,1). The discount is essential because the rewards tend to be much more significant in the beginning than in the end. And it makes perfect sense. The next step is to solve the problem. To do that we define that the goal of the learning task is: The agent needs to learn which action to perform from a given state that maximized the cumulative reward over time. Or to learn the Policy π: S-&gt;A. The policy is just a mapping between a state and an action. To sum all the above, we use the following equation: , where V (Value) is the expected long-term reward achieved by a policy (π) from a state (s). You still with me? If you are, let’s pause for 5 seconds cause this was a bit overwhelming: 1…2…3…4…5 Now that we regain our mental clarity, let recap. We have the definition of the problem as a Markov Decision Process and we have our goal to learn the best Policy or the best Value. How we proceed? We need an algorithm (Thank you, Sherlock…) Well, there is an abundance of developed RL algorithms over the years. Each algorithm focuses on a different thing, whether it is to maximize the value or the policy or both. Whether to use a model(e.g a neural network) to simulate the environment or not. Whether it will capture the reward on each step or the end. As you guessed, it is not very easy to categorize all those algorithms in classes, but that’s what I am about to do. As you can see we can classify RL algorithms in two big categories: Model-based and Model-free: Model-based These algorithms aim to learn how the environment works (its dynamics) from its observations and then plan a solution using that model. When they have a model, they use some planning method to find the best policy. They known to be data efficient, but they fail when the state space is too large. Try to build a model-based algorithm to play Go. Not gonna happen. Dynamic programming methods are an example of model-based methods, as they require the complete knowledge of the environment, such as transition probabilities and rewards. Model-free Model-free algorithms do not require to learn the environment and store all the combination of states and actions. The can be divided into two categories, based on the ultimate goal of the training. Policy-based methods try to find the optimal policy, whether it’s stochastic or deterministic. Algorithms like policy gradients and REINFORCE belong in this category. Their advantages are better convergence and effectiveness on high dimensional or continuous action spaces. Policy-based methods are essentially an optimization problem, where we find the maximum of a policy function. That’s why we also use algorithms like evolution strategies and hill climbing. Value-based methods, on the other hand, try to find the optimal value. A big part of this category is a family of algorithms called Q-learning, which learn to optimize the Q-Value. I plan to analyze Q-learning thoroughly on a next article because it is an essential aspect of Reinforcement learning. Other algorithms involve SARSA and value iteration. At the intersection of policy and value-based method, we find the Actor-Critic methods, where the goal is to optimize both the policy and the value function. And now to the cool part. In the past few years, there is a new kid in town. And it was inevitable to affect and enhance all the existing methods to solve Reinforcement Learning. I am sure you guessed it. Deep Learning. And thus, we have a new term to represent all those new research ideas. Deep Reinforcement Learning Deep neural networks have been used to model the dynamics of the environment(mode-based), to enhance policy searches (policy-based) and to approximate the Value function (value-based). Research on the last one (which is my favorite) has produced a model called Deep Q Network, which is responsible ,along with its many improvements, for some of the most astonishing breakthroughs around the area (take Atari for example). And to excite you, even more, we don’t just use simple Neural Networks but Convolutional, Recurrent and many else as well. Ok, I think that’s enough for the first contact with Reinforcement Learning. I just wanted to give you the basis behind the whole idea and present you an overview of all the important techniques implemented over the years. But also, to give you a hint of what’s next for the field. Reinforcement learning has applications both in industry and in research. To name a few it has been used for: Robotics control, Optimizing chemical reactions, Recommendation systems, Advertising, Product design, Supply chain optimization, Stock trading. I could go on forever. Its probably the most exciting area of AI right now and in my opinion, it has all the rights to be. Until my next article, stay tuned. For the next article in the Reinforcement Learning Journey, click here.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href=""> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( /assets/img/posts/RL.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">The secrets behind Reinforcement Learning</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Sep 23, 2018</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            7 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="the-secrets-behind-reinforcement-learning">The secrets behind Reinforcement Learning</h1>

<p>Bots that play Dota2, AI that beat the best Go players in the world, computers
that excel at Doom. What’s going on? Is there a reason why the AI community has
been so busy playing games?</p>

<p>Let me put it that way. If you want a robot to learn how to walk what do you do?
You build one, program it and release it on the streets of New York? Of course
not. You build a simulation, a game, and you use that virtual space to teach it
how to move around it. Zero cost, zero risks. That’s why games are so useful in
research areas. But how do you teach it to walk? The answer is the topic of
today’s article and is probably the most exciting field of Machine learning at
the time:</p>

<p>You probably knew that there are two types of machine learning. Supervised and
unsupervised. Well, there is a third one, called Reinforcement Learning. RL is
arguably the most difficult area of ML to understand cause there are so many,
many things going on at the same time. I’ll try to simplify as much as I can
because it is a really astonishing area and you should definitely know about it.
But let me warn you. It involves complex thinking and 100% focus to grasp it.
And some math. So, take a deep breath and let’s dive in:</p>

<h2 id="markov-decision-processes">Markov Decision Processes</h2>

<p>Reinforcement learning is a trial and error process where an AI (<strong>agent</strong>)
performs a number of <strong>actions</strong> in an <strong>environment</strong>. Each unique moment the
agent has a <strong>state</strong> and acts from this given state to a new one. This
particular action may on may not has a <strong>reward.</strong> Therefore, we can say that
each learning epoch (or an episode) can be represented as a sequence of states,
actions, and rewards. Each state depends only on the previous states and actions
and as the environment is inherently stochastic (we don’t know the state that
comes next), this process satisfies the <a href="https://en.wikipedia.org/wiki/Markov_property">Markov
property</a>. The Markov property
says that the conditional probability distribution of future states of the
process depends only upon the present state, not on the sequence of events that
preceded it. The whole process is referred to as <a href="https://en.wikipedia.org/wiki/Markov_decision_process">a Markov Decision
Process</a>.</p>

<p><img src="/assets/img/posts/RL.jpg" alt="RL" /></p>

<p>Let’s see a real example using Super Mario. In this case:</p>

<ul>
  <li>
    <p>The agent is, of course, the beloved Mario</p>
  </li>
  <li>
    <p>The state is the current situation (let’s say the frame of our screen)</p>
  </li>
  <li>
    <p>The actions are: left, right movement and jump</p>
  </li>
  <li>
    <p>The environment is the virtual world of each level;</p>
  </li>
  <li>
    <p>And the reward is whether Mario is alive or dead.</p>
  </li>
</ul>

<p>Ok, we have properly defined the problem. What’s next? We need a solution. But
first, we need a way to evaluate how good the solution is?</p>

<p>What I am saying is that the reward on each episode it’s not enough. Imagine a
Mario game where the Mario is controlled by an agent. He is receiving constantly
positive rewards through the whole level by just before the final flag, he is
killed by one Hammer Bro (I hate those guys). You see that each individual
reward is not enough for us to win the game. We need a reward that captures the
whole level. This is where the term of the <strong>discounted cumulative expected
reward</strong> comes into play.</p>

<p><img src="/assets/img/posts/RL_reward.jpg" alt="reward" /></p>

<p>It is nothing more than the sum of all rewards discounted by a factor gamma,
where gamma belongs to [0,1). The discount is essential because the rewards tend
to be much more significant in the beginning than in the end. And it makes
perfect sense.</p>

<p>The next step is to solve the problem. To do that we define that the goal of the
learning task is: The agent needs to learn which action to perform from a given
state that maximized the cumulative reward over time. Or to learn the <strong>Policy
π: S-&gt;A.</strong> The policy is just a mapping between a state and an action.</p>

<p>To sum all the above, we use the following equation:</p>

<p><img src="/assets/img/posts/RL_value.jpg" alt="value" /></p>

<p>, where V (<strong>Value</strong>) is the <strong>expected</strong> long-term reward achieved by a policy
(π) from a state (s).</p>

<p>You still with me? If you are, let’s pause for 5 seconds cause this was a bit
overwhelming:</p>

<p>1…2…3…4…5</p>

<p>Now that we regain our mental clarity, let recap. We have the definition of the
problem as a Markov Decision Process and we have our goal to learn the best
Policy or the best Value. How we proceed?</p>

<p>We need an algorithm (Thank you, Sherlock…)</p>

<p>Well, there is an abundance of developed RL algorithms over the years. Each
algorithm focuses on a different thing, whether it is to maximize the value or
the policy or both. Whether to use a model(e.g a neural network) to simulate the
environment or not. Whether it will capture the reward on each step or the end.
As you guessed, it is not very easy to categorize all those algorithms in
classes, but that’s what I am about to do.</p>

<p><img src="/assets/img/posts/RL_algorithms.jpg" alt="RL_algorithms" /></p>

<p>As you can see we can classify RL algorithms in two big categories: Model-based
and Model-free:</p>

<h2 id="model-based">Model-based</h2>

<p>These algorithms aim to learn how the environment works (its dynamics) from its
observations and then plan a solution using that model. When they have a model,
they use some planning method to find the best policy. They known to be data
efficient, but they fail when the state space is too large. Try to build a
model-based algorithm to play Go. Not gonna happen.</p>

<p><a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a> methods
are an example of model-based methods, as they require the complete knowledge of
the environment, such as transition probabilities and rewards.</p>

<h2 id="model-free">Model-free</h2>

<p>Model-free algorithms do not require to learn the environment and store all the
combination of states and actions. The can be divided into two categories, based
on the ultimate goal of the training.</p>

<p><strong>Policy-based</strong> methods try to find the optimal policy, whether it’s stochastic
or deterministic. Algorithms like policy gradients and REINFORCE belong in this
category. Their advantages are better convergence and effectiveness on high
dimensional or continuous action spaces.</p>

<p>Policy-based methods are essentially an optimization problem, where we find the
maximum of a policy function. That’s why we also use algorithms like evolution
strategies and hill climbing.</p>

<p><strong>Value-based</strong> methods, on the other hand, try to find the optimal value. A big
part of this category is a family of algorithms called
<a href="https://en.wikipedia.org/wiki/Q-learning">Q-learning</a>, which learn to optimize
the Q-Value. I plan to analyze Q-learning thoroughly on a next article because
it is an essential aspect of Reinforcement learning. Other algorithms involve
SARSA and value iteration.</p>

<p>At the intersection of policy and value-based method, we find the
<strong>Actor-Critic</strong> methods, where the goal is to optimize both the policy and the
value function.</p>

<p>And now to the cool part. In the past few years, there is a new kid in town. And
it was inevitable to affect and enhance all the existing methods to solve
Reinforcement Learning. I am sure you guessed it. Deep Learning. And thus, we
have a new term to represent all those new research ideas.</p>

<h2 id="deep-reinforcement-learning">Deep Reinforcement Learning</h2>

<p><img src="/assets/img/posts/DRL.jpg" alt="DRL" /></p>

<p>Deep neural networks have been used to model the dynamics of the
environment(mode-based), to enhance policy searches (policy-based) and to
approximate the Value function (value-based). Research on the last one (which is
my favorite) has produced a model called <a href="https://deepmind.com/research/dqn/">Deep Q
Network</a>, which is responsible ,along with
its many improvements, for some of the most astonishing breakthroughs around the
area (take Atari for example). And to excite you, even more, we don’t just use
simple Neural Networks but Convolutional, Recurrent and many else as well.</p>

<p>Ok, I think that’s enough for the first contact with Reinforcement Learning. I
just wanted to give you the basis behind the whole idea and present you an
overview of all the important techniques implemented over the years. But also,
to give you a hint of what’s next for the field.</p>

<p>Reinforcement learning has applications both in industry and in research. To
name a few it has been used for: Robotics control, Optimizing chemical
reactions, Recommendation systems, Advertising, Product design, Supply chain
optimization, Stock trading. I could go on forever.</p>

<p>Its probably the most exciting area of AI right now and in my opinion, it has
all the rights to be.</p>

<p>Until my next article, stay tuned.</p>

<p>For the next article in the Reinforcement Learning Journey, click <a href="https://sergioskar.github.io/Deep_Q_Learning/">here</a>.</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Generative_Artificial_Intelligence/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            Decrypt Generative Artifici...
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Deep_Q_Learning/">
            Deep Q Learning
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="/lib/jquery/jquery.min.js"></script>
 <script src="/lib/jquery/jquery-migrate.min.js"></script>
 <script src="/lib/popper/popper.min.js"></script>
 <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="/lib/easing/easing.min.js"></script>
 <script src="/lib/counterup/jquery.waypoints.min.js"></script>
 <script src="/lib/counterup/jquery.counterup.js"></script>
 <script src="/lib/lightbox/js/lightbox.min.js"></script>
 <script src="/lib/typed/typed.min.js"></script>
 <script src="/js/scripts.js"></script>
 <script src="/js/contact_form.js"></script>



  </body>

</html>



