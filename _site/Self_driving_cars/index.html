
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Self-driving cars using Deep Learning | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Self-driving cars using Deep Learning" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Self-driving cars using Deep Learning Self- driving cars will be without a doubt the standard way of transportation in the future. Major companies from Uber and Google to Toyota and General Motors are willing to spend millions of dollars to make them a reality, as the future market is predicted to worth trillions. In the past years, we have seen an enormous evolution in the area with cars from Uber, Tesla, Waymo to have a total of 8 million miles in their records. Of course, self-driving cars are now a reality due to many different technological advancements both in hardware and in software. LIDAR sensors, cameras, GPS, ultrasonic sensors are working together to receive data from every possible source. Those data are analyzed in real time using advanced algorithms, making the autopilot functionality possible. https://www.technologyreview.com/s/609674/whats-driving-autonomous-vehicles/ There are 5 essential steps to form the self-driving pipeline with the following order: Localization Perception Prediction Planning Control Localization is basically how an autonomous vehicle knows exactly where it is in the world. In this step, they get the data from all the above-mentioned sensors (sensor fusion) and use a technique called Kalman filters to find their position with the highest possible accuracy. Kalman filter is a probabilistic method that use measurements over time to estimate the state of the object’s position. Another widely used technique is particle filters. Perception is how cars sense and understand their environment. Here is where computer vision and neural networks come into play. But more on that later. In the prediction step, cars predict the behavior of every object (vehicle or human) in their surroundings. How they will move, in which direction, at which speed, what trajectory they will follow. One of the most common modes used here is a recurrent neural network, as it can learn from past behavior and forecast the future. Path planning is self-explainable. It is where that car plans the route to follow or in other words generates its trajectory. This is accomplished with search algorithms (like A*), Lattice planning and Reinforcement Learning. Finally, control engineers take it from here. They use the trajectory generated in the previous step to change accordingly the steering, acceleration and breaks of the car. The most common method is PID Control but there are a few others such as Linear quadratic regulator(LQR) and Model predictive control(MPC) By the way, if you want to learn more check the two awesome courses offered by Udacity for free: https://classroom.udacity.com/courses/cs373 https://classroom.udacity.com/courses/ud0419 Well, I think it’s now time to build an autonomous car by ourselves. Ok, not all of it. But what we can do is use a driving simulator and record what the camera sees. Then we can feed those frames into a neural network and hopefully the car might be able to learn how to drive on its own. Let’s see… We will use Udacity’s open sourced Self-Driving Car Simulator. To use it, you need to install Unity game engine. Now the fun part: It goes without saying that I spend about an hour recording the frames. It was some serious work guys. I was not fooling around. Anyway, now the simulator has produced 1551 frames from 3 different angles and also logged the steering angle, the speed, the throttle and the break for each of the different 517 states. Before we build the model in keras, we have to read the data and split them into the training and test sets. def load_data(): data_df = pd.read_csv(os.path.join(os.getcwd(),data_dir, &#39;driving_log.csv&#39;), names=[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;, &#39;steering&#39;, &#39;throttle&#39;, &#39;reverse&#39;, &#39;speed&#39;]) X = data_df[[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;]].values y = data_df[&#39;steering&#39;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test After that, we will build our model which has 5 Convolutional, one Dropout and 4 Dense layers. def build_model(): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) model.add(Conv2D(24, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(36, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(48, kernel_size=(5, 5),strides=(2,2),activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(100, activation=&#39;elu&#39;)) model.add(Dense(50, activation=&#39;elu&#39;)) model.add(Dense(10, activation=&#39;elu&#39;)) model.add(Dense(1)) #model.summary() return model The network will output only one value, the steering angle. Before we pass the inputs on the model, we should do a little preprocessing. Note that this is done with OpenCV, an open-sourced library that is build for image and video manipulation. First of all we have to produce more data and we will do that by augment our existing. We can for example flip the existing images, translate them, add random shadow or change their brightness. image, steering_angle = choose_image(data_dir, center, left, right, steering_angle) image, steering_angle = random_flip(image, steering_angle) image, steering_angle = random_translate(image, steering_angle, range_x, range_y) image = random_shadow(image) image = random_brightness(image) Next, we have to make sure to crop and resize the images in order to fit into our network. def preprocess(image): image = crop(image) image = resize(image) image = rgb2yuv(image) return image Training time: def train_model(model, X_train, X_valid, y_train, y_valid): model.compile(loss=&#39;mean_squared_error&#39;, optimizer=Adam(lr=0.001)) #Fits the model on data generated batch-by-batch by a Python generator. model.fit_generator(batch_generator(data_dir, X_train, y_train, batch_size, True), steps_per_epoch, num_epochs, verbose=1, validation_data=batch_generator(data_dir, X_valid, y_valid, batch_size, False), validation_steps=40 ) Now we have the trained model. It has essentially cloned our driving behavior. Let’s see how we did it. To do that, we need a simple server (socketio server) to send the model prediction to the simulator in real-time. I am not going to get into many details about the server stuff. What’s important is the part that we predict the steering angle using the frames and logs generated by the simulator in real time. steering_angle = float(data[&quot;steering_angle&quot;]) throttle = float(data[&quot;throttle&quot;]) speed = float(data[&quot;speed&quot;]) image = Image.open(BytesIO(base64.b64decode(data[&quot;image&quot;]))) image = np.asarray(image) image = preprocess_data.preprocess(image) image = np.array([image]) steering_angle = float(model.predict(image, batch_size=1)) throttle = 1.0 - steering_angle ** 2 - (speed / speed_limit) ** 2 #send prediction to the simulator send_control(steering_angle, throttle) And the result: Not bad. Not bad at all. We actually did it. I think that Udacity’s emulator is the easiest way for someone to start learning about self-driving vehicles. To wrap up, autonomous cars have already started being mainstream and there is no doubt that they become commonplace sooner than most of us think. It is extremely complex to build one as it requires so many different components from sensors to software. But here we just did a very very small first step. The major thing is that the future is here. And it is exciting…" />
<meta property="og:description" content="Self-driving cars using Deep Learning Self- driving cars will be without a doubt the standard way of transportation in the future. Major companies from Uber and Google to Toyota and General Motors are willing to spend millions of dollars to make them a reality, as the future market is predicted to worth trillions. In the past years, we have seen an enormous evolution in the area with cars from Uber, Tesla, Waymo to have a total of 8 million miles in their records. Of course, self-driving cars are now a reality due to many different technological advancements both in hardware and in software. LIDAR sensors, cameras, GPS, ultrasonic sensors are working together to receive data from every possible source. Those data are analyzed in real time using advanced algorithms, making the autopilot functionality possible. https://www.technologyreview.com/s/609674/whats-driving-autonomous-vehicles/ There are 5 essential steps to form the self-driving pipeline with the following order: Localization Perception Prediction Planning Control Localization is basically how an autonomous vehicle knows exactly where it is in the world. In this step, they get the data from all the above-mentioned sensors (sensor fusion) and use a technique called Kalman filters to find their position with the highest possible accuracy. Kalman filter is a probabilistic method that use measurements over time to estimate the state of the object’s position. Another widely used technique is particle filters. Perception is how cars sense and understand their environment. Here is where computer vision and neural networks come into play. But more on that later. In the prediction step, cars predict the behavior of every object (vehicle or human) in their surroundings. How they will move, in which direction, at which speed, what trajectory they will follow. One of the most common modes used here is a recurrent neural network, as it can learn from past behavior and forecast the future. Path planning is self-explainable. It is where that car plans the route to follow or in other words generates its trajectory. This is accomplished with search algorithms (like A*), Lattice planning and Reinforcement Learning. Finally, control engineers take it from here. They use the trajectory generated in the previous step to change accordingly the steering, acceleration and breaks of the car. The most common method is PID Control but there are a few others such as Linear quadratic regulator(LQR) and Model predictive control(MPC) By the way, if you want to learn more check the two awesome courses offered by Udacity for free: https://classroom.udacity.com/courses/cs373 https://classroom.udacity.com/courses/ud0419 Well, I think it’s now time to build an autonomous car by ourselves. Ok, not all of it. But what we can do is use a driving simulator and record what the camera sees. Then we can feed those frames into a neural network and hopefully the car might be able to learn how to drive on its own. Let’s see… We will use Udacity’s open sourced Self-Driving Car Simulator. To use it, you need to install Unity game engine. Now the fun part: It goes without saying that I spend about an hour recording the frames. It was some serious work guys. I was not fooling around. Anyway, now the simulator has produced 1551 frames from 3 different angles and also logged the steering angle, the speed, the throttle and the break for each of the different 517 states. Before we build the model in keras, we have to read the data and split them into the training and test sets. def load_data(): data_df = pd.read_csv(os.path.join(os.getcwd(),data_dir, &#39;driving_log.csv&#39;), names=[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;, &#39;steering&#39;, &#39;throttle&#39;, &#39;reverse&#39;, &#39;speed&#39;]) X = data_df[[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;]].values y = data_df[&#39;steering&#39;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test After that, we will build our model which has 5 Convolutional, one Dropout and 4 Dense layers. def build_model(): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) model.add(Conv2D(24, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(36, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(48, kernel_size=(5, 5),strides=(2,2),activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(100, activation=&#39;elu&#39;)) model.add(Dense(50, activation=&#39;elu&#39;)) model.add(Dense(10, activation=&#39;elu&#39;)) model.add(Dense(1)) #model.summary() return model The network will output only one value, the steering angle. Before we pass the inputs on the model, we should do a little preprocessing. Note that this is done with OpenCV, an open-sourced library that is build for image and video manipulation. First of all we have to produce more data and we will do that by augment our existing. We can for example flip the existing images, translate them, add random shadow or change their brightness. image, steering_angle = choose_image(data_dir, center, left, right, steering_angle) image, steering_angle = random_flip(image, steering_angle) image, steering_angle = random_translate(image, steering_angle, range_x, range_y) image = random_shadow(image) image = random_brightness(image) Next, we have to make sure to crop and resize the images in order to fit into our network. def preprocess(image): image = crop(image) image = resize(image) image = rgb2yuv(image) return image Training time: def train_model(model, X_train, X_valid, y_train, y_valid): model.compile(loss=&#39;mean_squared_error&#39;, optimizer=Adam(lr=0.001)) #Fits the model on data generated batch-by-batch by a Python generator. model.fit_generator(batch_generator(data_dir, X_train, y_train, batch_size, True), steps_per_epoch, num_epochs, verbose=1, validation_data=batch_generator(data_dir, X_valid, y_valid, batch_size, False), validation_steps=40 ) Now we have the trained model. It has essentially cloned our driving behavior. Let’s see how we did it. To do that, we need a simple server (socketio server) to send the model prediction to the simulator in real-time. I am not going to get into many details about the server stuff. What’s important is the part that we predict the steering angle using the frames and logs generated by the simulator in real time. steering_angle = float(data[&quot;steering_angle&quot;]) throttle = float(data[&quot;throttle&quot;]) speed = float(data[&quot;speed&quot;]) image = Image.open(BytesIO(base64.b64decode(data[&quot;image&quot;]))) image = np.asarray(image) image = preprocess_data.preprocess(image) image = np.array([image]) steering_angle = float(model.predict(image, batch_size=1)) throttle = 1.0 - steering_angle ** 2 - (speed / speed_limit) ** 2 #send prediction to the simulator send_control(steering_angle, throttle) And the result: Not bad. Not bad at all. We actually did it. I think that Udacity’s emulator is the easiest way for someone to start learning about self-driving vehicles. To wrap up, autonomous cars have already started being mainstream and there is no doubt that they become commonplace sooner than most of us think. It is extremely complex to build one as it requires so many different components from sensors to software. But here we just did a very very small first step. The major thing is that the future is here. And it is exciting…" />
<link rel="canonical" href="/Self_driving_cars/" />
<meta property="og:url" content="/Self_driving_cars/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-09-04T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"/Self_driving_cars/","headline":"Self-driving cars using Deep Learning","dateModified":"2018-09-04T00:00:00+03:00","datePublished":"2018-09-04T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"/Self_driving_cars/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"Self-driving cars using Deep Learning Self- driving cars will be without a doubt the standard way of transportation in the future. Major companies from Uber and Google to Toyota and General Motors are willing to spend millions of dollars to make them a reality, as the future market is predicted to worth trillions. In the past years, we have seen an enormous evolution in the area with cars from Uber, Tesla, Waymo to have a total of 8 million miles in their records. Of course, self-driving cars are now a reality due to many different technological advancements both in hardware and in software. LIDAR sensors, cameras, GPS, ultrasonic sensors are working together to receive data from every possible source. Those data are analyzed in real time using advanced algorithms, making the autopilot functionality possible. https://www.technologyreview.com/s/609674/whats-driving-autonomous-vehicles/ There are 5 essential steps to form the self-driving pipeline with the following order: Localization Perception Prediction Planning Control Localization is basically how an autonomous vehicle knows exactly where it is in the world. In this step, they get the data from all the above-mentioned sensors (sensor fusion) and use a technique called Kalman filters to find their position with the highest possible accuracy. Kalman filter is a probabilistic method that use measurements over time to estimate the state of the object’s position. Another widely used technique is particle filters. Perception is how cars sense and understand their environment. Here is where computer vision and neural networks come into play. But more on that later. In the prediction step, cars predict the behavior of every object (vehicle or human) in their surroundings. How they will move, in which direction, at which speed, what trajectory they will follow. One of the most common modes used here is a recurrent neural network, as it can learn from past behavior and forecast the future. Path planning is self-explainable. It is where that car plans the route to follow or in other words generates its trajectory. This is accomplished with search algorithms (like A*), Lattice planning and Reinforcement Learning. Finally, control engineers take it from here. They use the trajectory generated in the previous step to change accordingly the steering, acceleration and breaks of the car. The most common method is PID Control but there are a few others such as Linear quadratic regulator(LQR) and Model predictive control(MPC) By the way, if you want to learn more check the two awesome courses offered by Udacity for free: https://classroom.udacity.com/courses/cs373 https://classroom.udacity.com/courses/ud0419 Well, I think it’s now time to build an autonomous car by ourselves. Ok, not all of it. But what we can do is use a driving simulator and record what the camera sees. Then we can feed those frames into a neural network and hopefully the car might be able to learn how to drive on its own. Let’s see… We will use Udacity’s open sourced Self-Driving Car Simulator. To use it, you need to install Unity game engine. Now the fun part: It goes without saying that I spend about an hour recording the frames. It was some serious work guys. I was not fooling around. Anyway, now the simulator has produced 1551 frames from 3 different angles and also logged the steering angle, the speed, the throttle and the break for each of the different 517 states. Before we build the model in keras, we have to read the data and split them into the training and test sets. def load_data(): data_df = pd.read_csv(os.path.join(os.getcwd(),data_dir, &#39;driving_log.csv&#39;), names=[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;, &#39;steering&#39;, &#39;throttle&#39;, &#39;reverse&#39;, &#39;speed&#39;]) X = data_df[[&#39;center&#39;, &#39;left&#39;, &#39;right&#39;]].values y = data_df[&#39;steering&#39;].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) return X_train, X_test, y_train, y_test After that, we will build our model which has 5 Convolutional, one Dropout and 4 Dense layers. def build_model(): model = Sequential() model.add(Lambda(lambda x: x/127.5-1.0, input_shape=INPUT_SHAPE)) model.add(Conv2D(24, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(36, kernel_size=(5, 5),strides=(2,2) ,activation=&#39;elu&#39;)) model.add(Conv2D(48, kernel_size=(5, 5),strides=(2,2),activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Conv2D(64, kernel_size=(3, 3), activation=&#39;elu&#39;)) model.add(Dropout(0.5)) model.add(Flatten()) model.add(Dense(100, activation=&#39;elu&#39;)) model.add(Dense(50, activation=&#39;elu&#39;)) model.add(Dense(10, activation=&#39;elu&#39;)) model.add(Dense(1)) #model.summary() return model The network will output only one value, the steering angle. Before we pass the inputs on the model, we should do a little preprocessing. Note that this is done with OpenCV, an open-sourced library that is build for image and video manipulation. First of all we have to produce more data and we will do that by augment our existing. We can for example flip the existing images, translate them, add random shadow or change their brightness. image, steering_angle = choose_image(data_dir, center, left, right, steering_angle) image, steering_angle = random_flip(image, steering_angle) image, steering_angle = random_translate(image, steering_angle, range_x, range_y) image = random_shadow(image) image = random_brightness(image) Next, we have to make sure to crop and resize the images in order to fit into our network. def preprocess(image): image = crop(image) image = resize(image) image = rgb2yuv(image) return image Training time: def train_model(model, X_train, X_valid, y_train, y_valid): model.compile(loss=&#39;mean_squared_error&#39;, optimizer=Adam(lr=0.001)) #Fits the model on data generated batch-by-batch by a Python generator. model.fit_generator(batch_generator(data_dir, X_train, y_train, batch_size, True), steps_per_epoch, num_epochs, verbose=1, validation_data=batch_generator(data_dir, X_valid, y_valid, batch_size, False), validation_steps=40 ) Now we have the trained model. It has essentially cloned our driving behavior. Let’s see how we did it. To do that, we need a simple server (socketio server) to send the model prediction to the simulator in real-time. I am not going to get into many details about the server stuff. What’s important is the part that we predict the steering angle using the frames and logs generated by the simulator in real time. steering_angle = float(data[&quot;steering_angle&quot;]) throttle = float(data[&quot;throttle&quot;]) speed = float(data[&quot;speed&quot;]) image = Image.open(BytesIO(base64.b64decode(data[&quot;image&quot;]))) image = np.asarray(image) image = preprocess_data.preprocess(image) image = np.array([image]) steering_angle = float(model.predict(image, batch_size=1)) throttle = 1.0 - steering_angle ** 2 - (speed / speed_limit) ** 2 #send prediction to the simulator send_control(steering_angle, throttle) And the result: Not bad. Not bad at all. We actually did it. I think that Udacity’s emulator is the easiest way for someone to start learning about self-driving vehicles. To wrap up, autonomous cars have already started being mainstream and there is no doubt that they become commonplace sooner than most of us think. It is extremely complex to build one as it requires so many different components from sensors to software. But here we just did a very very small first step. The major thing is that the future is here. And it is exciting…","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href=""> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( /assets/img/posts/sdc_sensors.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">Self-driving cars using Deep Learning</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Sep 4, 2018</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            9 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="self-driving-cars-using-deep-learning">Self-driving cars using Deep Learning</h1>

<p>Self- driving cars will be without a doubt the standard way of transportation in
the future. Major companies from Uber and Google to Toyota and General Motors
are willing to spend millions of dollars to make them a reality, as the future
market is predicted to worth trillions. In the past years, we have seen an
enormous evolution in the area with cars from Uber, Tesla, Waymo to have a total
of 8 million miles in their records.</p>

<p>Of course, self-driving cars are now a reality due to many different
technological advancements both in hardware and in software. LIDAR sensors,
cameras, GPS, ultrasonic sensors are working together to receive data from every
possible source. Those data are analyzed in real time using advanced algorithms,
making the autopilot functionality possible.</p>

<p><img src="/assets/img/posts/sdc_sensors.jpg" alt="Self driving cars sensors" /></p>
<blockquote>
  <p><em>https://www.technologyreview.com/s/609674/whats-driving-autonomous-vehicles/</em></p>
</blockquote>

<p>There are 5 essential steps to form the self-driving pipeline with the following
order:</p>

<ol>
  <li>
    <p>Localization</p>
  </li>
  <li>
    <p>Perception</p>
  </li>
  <li>
    <p>Prediction</p>
  </li>
  <li>
    <p>Planning</p>
  </li>
  <li>
    <p>Control</p>
  </li>
</ol>

<p>Localization is basically how an autonomous vehicle knows exactly where it
is in the world. In this step, they get the data from all the
above-mentioned sensors (sensor fusion) and use a technique called Kalman
filters to find their position with the highest possible accuracy. <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman
filter</a> is a probabilistic
method that use measurements over time to estimate the state of the object’s
position. Another widely used technique is <a href="https://en.wikipedia.org/wiki/Particle_filter#Particle_filters">particle
filters</a>.</p>

<p>Perception is how cars sense and understand their environment. Here is where
computer vision and neural networks come into play. But more on that later.</p>

<p>In the prediction step, cars predict the behavior of every object (vehicle
or human) in their surroundings. How they will move, in which direction, at
which speed, what trajectory they will follow. One of the most common modes
used here is a recurrent neural network, as it can learn from past behavior
and forecast the future.</p>

<p>Path planning is self-explainable. It is where that car plans the route to
follow or in other words generates its trajectory. This is accomplished with
search algorithms (like
<a href="https://en.wikipedia.org/wiki/A*_search_algorithm">A*</a>), Lattice planning
and Reinforcement Learning.</p>

<p>Finally, control engineers take it from here. They use the trajectory
generated in the previous step to change accordingly the steering,
acceleration and breaks of the car. The most common method is
<a href="https://en.wikipedia.org/wiki/PID_controller">PID</a> Control but there are a
few others such as <a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">Linear quadratic regulator(LQR)</a>
and <a href="https://en.wikipedia.org/wiki/Model_predictive_control">Model predictive control(MPC)</a></p>

<p>By the way, if you want to learn more check the two awesome courses offered
by Udacity for free:</p>

<ul>
  <li>
    <p><a href="https://classroom.udacity.com/courses/cs373">https://classroom.udacity.com/courses/cs373</a></p>
  </li>
  <li>
    <p><a href="https://classroom.udacity.com/courses/ud0419">https://classroom.udacity.com/courses/ud0419</a></p>
  </li>
</ul>

<p>Well, I think it’s now time to build an autonomous car by ourselves. Ok, not all
of it. But what we can do is use a driving simulator and record what the camera
sees. Then we can feed those frames into a neural network and hopefully the car
might be able to learn how to drive on its own. Let’s see…</p>

<p>We will use Udacity’s open sourced <a href="https://github.com/udacity/self-driving-car-sim">Self-Driving Car
Simulator</a>. To use it, you need
to install Unity game engine. Now the fun part:</p>

<p><img src="/assets/img/posts/sdc_simulator.jpg" alt="Udacity simulator" /></p>

<p>It goes without saying that I spend about an hour recording the frames. It was
some serious work guys. I was not fooling around.</p>

<p>Anyway, now the simulator has produced 1551 frames from 3 different angles and
also logged the steering angle, the speed, the throttle and the break for each
of the different 517 states.</p>

<p>Before we build the model in keras, we have to read the data and split them into
the training and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">load_data</span><span class="p">():</span>
    <span class="n">data_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">(),</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'driving_log.csv'</span><span class="p">),</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">'center'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">,</span> <span class="s">'steering'</span><span class="p">,</span> <span class="s">'throttle'</span><span class="p">,</span> <span class="s">'reverse'</span><span class="p">,</span> <span class="s">'speed'</span><span class="p">])</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">[[</span><span class="s">'center'</span><span class="p">,</span> <span class="s">'left'</span><span class="p">,</span> <span class="s">'right'</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data_df</span><span class="p">[</span><span class="s">'steering'</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span>
</code></pre></div></div>

<p>After that, we will build our model which has 5 Convolutional, one Dropout and 4
Dense layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_model</span><span class="p">():</span>

    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">/</span><span class="mf">127.5</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="n">INPUT_SHAPE</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">24</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">36</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="p">,</span><span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">48</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span><span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">),</span><span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'elu'</span><span class="p">))</span>
    <span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
    <span class="c1">#model.summary()
</span>    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>
<p>The network will output only one value, the steering angle.</p>

<p>Before we pass the inputs on the model, we should do a little preprocessing. Note that this is done with OpenCV, an open-sourced library that is build for image and video manipulation.</p>

<p>First of all we have to produce more data and we will do that by augment our existing. We can for example flip the existing images, translate them, add random shadow or change their brightness.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">image</span><span class="p">,</span> <span class="n">steering_angle</span> <span class="o">=</span> <span class="n">choose_image</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">right</span><span class="p">,</span> <span class="n">steering_angle</span><span class="p">)</span>
<span class="n">image</span><span class="p">,</span> <span class="n">steering_angle</span> <span class="o">=</span> <span class="n">random_flip</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">steering_angle</span><span class="p">)</span>
<span class="n">image</span><span class="p">,</span> <span class="n">steering_angle</span> <span class="o">=</span> <span class="n">random_translate</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">steering_angle</span><span class="p">,</span> <span class="n">range_x</span><span class="p">,</span> <span class="n">range_y</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">random_shadow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">random_brightness</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
</code></pre></div></div>
<p>Next, we have to make sure to crop and resize the images in order to fit into our network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">crop</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">rgb2yuv</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div>

<p>Training time:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'mean_squared_error'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">))</span>

    <span class="c1">#Fits the model on data generated batch-by-batch by a Python generator.
</span>    <span class="n">model</span><span class="o">.</span><span class="n">fit_generator</span><span class="p">(</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">True</span><span class="p">),</span>
                        <span class="n">steps_per_epoch</span><span class="p">,</span>
                        <span class="n">num_epochs</span><span class="p">,</span>
                        <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">validation_data</span><span class="o">=</span><span class="n">batch_generator</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">False</span><span class="p">),</span>
                        <span class="n">validation_steps</span><span class="o">=</span><span class="mi">40</span>
                        <span class="p">)</span>


</code></pre></div></div>

<p>Now we have the trained model. It has essentially cloned our driving behavior.
Let’s see how we did it. To do that, we need a simple server (socketio server)
to send the model prediction to the simulator in real-time. I am not going to
get into many details about the server stuff. What’s important is the part that
we predict the steering angle using the frames and logs generated by the
simulator in real time.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">steering_angle</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"steering_angle"</span><span class="p">])</span>
<span class="n">throttle</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"throttle"</span><span class="p">])</span>
<span class="n">speed</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"speed"</span><span class="p">])</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="nb">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">base64</span><span class="o">.</span><span class="n">b64decode</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">"image"</span><span class="p">])))</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">preprocess_data</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">image</span><span class="p">])</span>

<span class="n">steering_angle</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">throttle</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">steering_angle</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="p">(</span><span class="n">speed</span> <span class="o">/</span> <span class="n">speed_limit</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="c1">#send prediction to the simulator
</span><span class="n">send_control</span><span class="p">(</span><span class="n">steering_angle</span><span class="p">,</span> <span class="n">throttle</span><span class="p">)</span> 
</code></pre></div></div>
<p>And the result:</p>

<p><img src="https://media.giphy.com/media/YBIrZtvGPWDaiTqssz/giphy.gif" alt="Autonomous driving gif" /></p>

<p>Not bad. Not bad at all.</p>

<p>We actually did it. I think that Udacity’s emulator is the easiest way for someone to start learning about self-driving vehicles.</p>

<p>To wrap up, autonomous cars have already started being mainstream and there is no doubt that they become commonplace sooner than most of us think. It is extremely complex to build one as it requires so many different components from sensors to software. But here we just did a very very small first step.</p>

<p>The major thing is that the future is here. And it is exciting…</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Bitcon_prediction_LSTM/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            Predict Bitcoin price with ...
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Autoencoder/">
            How to Generate Images usin...
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="/lib/jquery/jquery.min.js"></script>
 <script src="/lib/jquery/jquery-migrate.min.js"></script>
 <script src="/lib/popper/popper.min.js"></script>
 <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="/lib/easing/easing.min.js"></script>
 <script src="/lib/counterup/jquery.waypoints.min.js"></script>
 <script src="/lib/counterup/jquery.counterup.js"></script>
 <script src="/lib/lightbox/js/lightbox.min.js"></script>
 <script src="/lib/typed/typed.min.js"></script>
 <script src="/js/scripts.js"></script>
 <script src="/js/contact_form.js"></script>



  </body>

</html>



