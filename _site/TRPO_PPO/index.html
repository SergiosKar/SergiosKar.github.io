
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Trust Region and Proximal policy optimization | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Trust Region and Proximal policy optimization" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Trust Region and Proximal policy optimization Welcome to another journey towards unraveling the secrets behind Reinforcement Learning. This time, we going to take a step back and return to policy optimization in order to introduce two new methods: trust region policy optimization (TRPO) and proximal policy optimization (PPO). Remember that in policy gradients techniques, we try to optimize a policy objective function (the expected accumulative reward) using gradient descent. Policy gradients are great for continuous and large spaces but suffer from some problems. High variance (which we address with Actor-critic models) Delayed reward problem Sample inefficiency Learning rate highly affects training Especially the last one troubled researchers for quite a long, because it is very hard to find a suitable learning rate for the whole optimization process. Small learning rate may cause vanishing gradients while large rate may cause exploding gradient. In general, we need a method to change the policy not too much but also not too little and even better to always improve our policy. One fundamental paper in this direction is : Trust region policy optimization (TRPO) To ensure that the policy won’t move too far, we add a constraint to our optimization problem in terms of making sure that the updated policy lies within a trust region. Trust regions are defined as the region in which the local approximations of the function are accurate. Ok, but what does that mean? In trust regions, we determine the maximum step size and then we find the local maximum of the policy within the region. By continuing the same process iteratively, we find the global maximum. We can also expand or shrink the region based on how good the new approximation is. That way we are certain that the new policies can be trustworthy of not leading to dramatically bad policy degradation. We can express mathematically the above constraint using KL divergence( which you can think of as a distance between two probabilities distributions): The KL divergence between the new and the old policy must be lower than the delta (δ), where delta is the size of the region. I could get into some math, but I think that this will only complicate things rather than clarify them. So essentially, we have just a constrained optimization problem. The question now is how we solve a constrained optimization problem? Using the Conjugate Gradient method. We can, of course, solve the problem analytically (natural gradient descent), but it is computational ineffective. If you dust of your knowledge on numerical mathematics, you might remember that the conjugate gradient method provides a numeric solution to a system of equations. That is way better from a computational perspective. So all we have to do is to approximate linearly the objective function and quadratically the constraint and let the conjugate gradient do its work. To wrap it all up, the algorithm has the following steps: We run a set of trajectories and collect the policies Estimate the advantages using advantage estimation algorithm Solve the constrained optimization problem using conjugate gradient Repeat Generally speaking, trust regions are considered pretty standard methods to approach optimization problems. The tricky part is to apply them in a reinforcement learning context in a way that provides an advantage over simple policy gradients. Although TRPO is a very powerful algorithm, it suffers from a significant problem: that bloody constraint, which adds additional overhead to our optimization problem. I mean it forces us to use the conjugate gradient method and baffled us with linear and quadratic approximations. Wouldn’t it be nice if the could somehow include the constraint directly into our optimization objective? As you might have guessed that is exactly what Proximal policy optimization does. Proximal policy optimization (PPO) This simple idea gave us a quite simpler and more intuitive algorithm than TRPO. And it turns out that it outperforms many of the existing techniques most of the time. So instead of adding a constraint separately, we incorporate it inside the objective function as a penalty (we subtract the KL divergence times a constant C from the function). As soon as we do that there is no need to solve a constrained problem and we can instead use a simple stochastic gradient descent in the above function. And the algorithm is transformed as follows: We run a set of trajectories and collect the policies Estimate the advantages using an advantage estimation algorithm Perform stochastic gradient descent on the objective function for a certain number of epochs Repeat A small caveat is that is hard to choose the coefficient C in a way that it works well over the whole course of optimization. To address that we update the coefficient based on how big or small the KL divergence is. If KL is too high, we increase it, or if it is too low, we decrease it. So this is it? This is the famous proximal policy optimization? Actually no. Sorry about that. It turns out that the function described above is not the same as the original paper. The authors found a way to improve this penalized version into a new, more robust objective function. Hey hey wait. What happened here? Actually, not much. Let me explain. One thing I intentionally left out so far is that fraction of probabilities over there, that seems to appear in TRPO also. Well, that is called importance sampling. We essentially have a new policy that we want to estimate and an old one that we use to collect the samples. With importance sampling, we can evaluate a new policy with the samples from the old one and improve sample efficiency. The ratio infers how different the two policies are and we denote is as r(theta). Using this ratio, we can construct a new objective function to clip the estimated advantage if the new policy is far away from the old one. And that’s exactly what the above equation does. If an action is a lot more likely under the new policy than the old one, we do not want to overdo the action update, so we clipped the objective function. If it is much less likely under the new policy than the old, the objective action is flattened out to prevent from overdoing the update once again. It may be just me, but I think I haven’t come across a simpler and cleaner reinforcement learning algorithm in a while. If you want to get into the rabbit hole, check out the baseline code from OpenAI, which will give you a crystal clear image of the whole algorithm and solve all your questions. Of course, it would be much better if you try to implement it from scratch all by yourself. It is not the easiest task, but why not. I think that’s it for now. Keep learning…" />
<meta property="og:description" content="Trust Region and Proximal policy optimization Welcome to another journey towards unraveling the secrets behind Reinforcement Learning. This time, we going to take a step back and return to policy optimization in order to introduce two new methods: trust region policy optimization (TRPO) and proximal policy optimization (PPO). Remember that in policy gradients techniques, we try to optimize a policy objective function (the expected accumulative reward) using gradient descent. Policy gradients are great for continuous and large spaces but suffer from some problems. High variance (which we address with Actor-critic models) Delayed reward problem Sample inefficiency Learning rate highly affects training Especially the last one troubled researchers for quite a long, because it is very hard to find a suitable learning rate for the whole optimization process. Small learning rate may cause vanishing gradients while large rate may cause exploding gradient. In general, we need a method to change the policy not too much but also not too little and even better to always improve our policy. One fundamental paper in this direction is : Trust region policy optimization (TRPO) To ensure that the policy won’t move too far, we add a constraint to our optimization problem in terms of making sure that the updated policy lies within a trust region. Trust regions are defined as the region in which the local approximations of the function are accurate. Ok, but what does that mean? In trust regions, we determine the maximum step size and then we find the local maximum of the policy within the region. By continuing the same process iteratively, we find the global maximum. We can also expand or shrink the region based on how good the new approximation is. That way we are certain that the new policies can be trustworthy of not leading to dramatically bad policy degradation. We can express mathematically the above constraint using KL divergence( which you can think of as a distance between two probabilities distributions): The KL divergence between the new and the old policy must be lower than the delta (δ), where delta is the size of the region. I could get into some math, but I think that this will only complicate things rather than clarify them. So essentially, we have just a constrained optimization problem. The question now is how we solve a constrained optimization problem? Using the Conjugate Gradient method. We can, of course, solve the problem analytically (natural gradient descent), but it is computational ineffective. If you dust of your knowledge on numerical mathematics, you might remember that the conjugate gradient method provides a numeric solution to a system of equations. That is way better from a computational perspective. So all we have to do is to approximate linearly the objective function and quadratically the constraint and let the conjugate gradient do its work. To wrap it all up, the algorithm has the following steps: We run a set of trajectories and collect the policies Estimate the advantages using advantage estimation algorithm Solve the constrained optimization problem using conjugate gradient Repeat Generally speaking, trust regions are considered pretty standard methods to approach optimization problems. The tricky part is to apply them in a reinforcement learning context in a way that provides an advantage over simple policy gradients. Although TRPO is a very powerful algorithm, it suffers from a significant problem: that bloody constraint, which adds additional overhead to our optimization problem. I mean it forces us to use the conjugate gradient method and baffled us with linear and quadratic approximations. Wouldn’t it be nice if the could somehow include the constraint directly into our optimization objective? As you might have guessed that is exactly what Proximal policy optimization does. Proximal policy optimization (PPO) This simple idea gave us a quite simpler and more intuitive algorithm than TRPO. And it turns out that it outperforms many of the existing techniques most of the time. So instead of adding a constraint separately, we incorporate it inside the objective function as a penalty (we subtract the KL divergence times a constant C from the function). As soon as we do that there is no need to solve a constrained problem and we can instead use a simple stochastic gradient descent in the above function. And the algorithm is transformed as follows: We run a set of trajectories and collect the policies Estimate the advantages using an advantage estimation algorithm Perform stochastic gradient descent on the objective function for a certain number of epochs Repeat A small caveat is that is hard to choose the coefficient C in a way that it works well over the whole course of optimization. To address that we update the coefficient based on how big or small the KL divergence is. If KL is too high, we increase it, or if it is too low, we decrease it. So this is it? This is the famous proximal policy optimization? Actually no. Sorry about that. It turns out that the function described above is not the same as the original paper. The authors found a way to improve this penalized version into a new, more robust objective function. Hey hey wait. What happened here? Actually, not much. Let me explain. One thing I intentionally left out so far is that fraction of probabilities over there, that seems to appear in TRPO also. Well, that is called importance sampling. We essentially have a new policy that we want to estimate and an old one that we use to collect the samples. With importance sampling, we can evaluate a new policy with the samples from the old one and improve sample efficiency. The ratio infers how different the two policies are and we denote is as r(theta). Using this ratio, we can construct a new objective function to clip the estimated advantage if the new policy is far away from the old one. And that’s exactly what the above equation does. If an action is a lot more likely under the new policy than the old one, we do not want to overdo the action update, so we clipped the objective function. If it is much less likely under the new policy than the old, the objective action is flattened out to prevent from overdoing the update once again. It may be just me, but I think I haven’t come across a simpler and cleaner reinforcement learning algorithm in a while. If you want to get into the rabbit hole, check out the baseline code from OpenAI, which will give you a crystal clear image of the whole algorithm and solve all your questions. Of course, it would be much better if you try to implement it from scratch all by yourself. It is not the easiest task, but why not. I think that’s it for now. Keep learning…" />
<link rel="canonical" href="/TRPO_PPO/" />
<meta property="og:url" content="/TRPO_PPO/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-11T00:00:00+02:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"/TRPO_PPO/","headline":"Trust Region and Proximal policy optimization","dateModified":"2019-01-11T00:00:00+02:00","datePublished":"2019-01-11T00:00:00+02:00","mainEntityOfPage":{"@type":"WebPage","@id":"/TRPO_PPO/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"Trust Region and Proximal policy optimization Welcome to another journey towards unraveling the secrets behind Reinforcement Learning. This time, we going to take a step back and return to policy optimization in order to introduce two new methods: trust region policy optimization (TRPO) and proximal policy optimization (PPO). Remember that in policy gradients techniques, we try to optimize a policy objective function (the expected accumulative reward) using gradient descent. Policy gradients are great for continuous and large spaces but suffer from some problems. High variance (which we address with Actor-critic models) Delayed reward problem Sample inefficiency Learning rate highly affects training Especially the last one troubled researchers for quite a long, because it is very hard to find a suitable learning rate for the whole optimization process. Small learning rate may cause vanishing gradients while large rate may cause exploding gradient. In general, we need a method to change the policy not too much but also not too little and even better to always improve our policy. One fundamental paper in this direction is : Trust region policy optimization (TRPO) To ensure that the policy won’t move too far, we add a constraint to our optimization problem in terms of making sure that the updated policy lies within a trust region. Trust regions are defined as the region in which the local approximations of the function are accurate. Ok, but what does that mean? In trust regions, we determine the maximum step size and then we find the local maximum of the policy within the region. By continuing the same process iteratively, we find the global maximum. We can also expand or shrink the region based on how good the new approximation is. That way we are certain that the new policies can be trustworthy of not leading to dramatically bad policy degradation. We can express mathematically the above constraint using KL divergence( which you can think of as a distance between two probabilities distributions): The KL divergence between the new and the old policy must be lower than the delta (δ), where delta is the size of the region. I could get into some math, but I think that this will only complicate things rather than clarify them. So essentially, we have just a constrained optimization problem. The question now is how we solve a constrained optimization problem? Using the Conjugate Gradient method. We can, of course, solve the problem analytically (natural gradient descent), but it is computational ineffective. If you dust of your knowledge on numerical mathematics, you might remember that the conjugate gradient method provides a numeric solution to a system of equations. That is way better from a computational perspective. So all we have to do is to approximate linearly the objective function and quadratically the constraint and let the conjugate gradient do its work. To wrap it all up, the algorithm has the following steps: We run a set of trajectories and collect the policies Estimate the advantages using advantage estimation algorithm Solve the constrained optimization problem using conjugate gradient Repeat Generally speaking, trust regions are considered pretty standard methods to approach optimization problems. The tricky part is to apply them in a reinforcement learning context in a way that provides an advantage over simple policy gradients. Although TRPO is a very powerful algorithm, it suffers from a significant problem: that bloody constraint, which adds additional overhead to our optimization problem. I mean it forces us to use the conjugate gradient method and baffled us with linear and quadratic approximations. Wouldn’t it be nice if the could somehow include the constraint directly into our optimization objective? As you might have guessed that is exactly what Proximal policy optimization does. Proximal policy optimization (PPO) This simple idea gave us a quite simpler and more intuitive algorithm than TRPO. And it turns out that it outperforms many of the existing techniques most of the time. So instead of adding a constraint separately, we incorporate it inside the objective function as a penalty (we subtract the KL divergence times a constant C from the function). As soon as we do that there is no need to solve a constrained problem and we can instead use a simple stochastic gradient descent in the above function. And the algorithm is transformed as follows: We run a set of trajectories and collect the policies Estimate the advantages using an advantage estimation algorithm Perform stochastic gradient descent on the objective function for a certain number of epochs Repeat A small caveat is that is hard to choose the coefficient C in a way that it works well over the whole course of optimization. To address that we update the coefficient based on how big or small the KL divergence is. If KL is too high, we increase it, or if it is too low, we decrease it. So this is it? This is the famous proximal policy optimization? Actually no. Sorry about that. It turns out that the function described above is not the same as the original paper. The authors found a way to improve this penalized version into a new, more robust objective function. Hey hey wait. What happened here? Actually, not much. Let me explain. One thing I intentionally left out so far is that fraction of probabilities over there, that seems to appear in TRPO also. Well, that is called importance sampling. We essentially have a new policy that we want to estimate and an old one that we use to collect the samples. With importance sampling, we can evaluate a new policy with the samples from the old one and improve sample efficiency. The ratio infers how different the two policies are and we denote is as r(theta). Using this ratio, we can construct a new objective function to clip the estimated advantage if the new policy is far away from the old one. And that’s exactly what the above equation does. If an action is a lot more likely under the new policy than the old one, we do not want to overdo the action update, so we clipped the objective function. If it is much less likely under the new policy than the old, the objective action is flattened out to prevent from overdoing the update once again. It may be just me, but I think I haven’t come across a simpler and cleaner reinforcement learning algorithm in a while. If you want to get into the rabbit hole, check out the baseline code from OpenAI, which will give you a crystal clear image of the whole algorithm and solve all your questions. Of course, it would be much better if you try to implement it from scratch all by yourself. It is not the easiest task, but why not. I think that’s it for now. Keep learning…","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href=""> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( /assets/img/posts/ppo_trpo.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">Trust Region and Proximal policy optimization</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Jan 11, 2019</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            6 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="trust-region-and-proximal-policy-optimization">Trust Region and Proximal policy optimization</h1>

<p>Welcome to another journey towards unraveling the secrets behind Reinforcement
Learning. This time, we going to take a step back and return to policy
optimization in order to introduce two new methods: trust region policy
optimization (TRPO) and proximal policy optimization (PPO). Remember that in
policy gradients techniques, we try to optimize a policy objective function (the
expected accumulative reward) using gradient descent. Policy gradients are great
for continuous and large spaces but suffer from some problems.</p>

<ul>
  <li>High variance (which we address with Actor-critic models)</li>
  <li>Delayed reward problem</li>
  <li>Sample inefficiency</li>
  <li>Learning rate highly affects training</li>
</ul>

<p>Especially the last one troubled researchers for quite a long, because it is
very hard to find a suitable learning rate for the whole optimization process.
Small learning rate may cause vanishing gradients while large rate may cause
exploding gradient. In general, we need a method to change the policy not too
much but also not too little and even better to always improve our policy. One
fundamental paper in this direction is :</p>

<h2 id="trust-region-policy-optimization-trpo">Trust region policy optimization (TRPO)</h2>

<p>To ensure that the policy won’t move too far, we add a constraint to our
optimization problem in terms of making sure that the updated policy lies within
a trust region. Trust regions are defined as the region in which the local
approximations of the function are accurate. Ok, but what does that mean? In
trust regions, we determine the maximum step size and then we find the local
maximum of the policy within the region. By continuing the same process
iteratively, we find the global maximum. We can also expand or shrink the region
based on how good the new approximation is. That way we are certain that the new
policies can be trustworthy of not leading to dramatically bad policy
degradation. We can express mathematically the above constraint using KL
divergence( which you can think of as a distance between two probabilities
distributions):</p>

<p><strong>The KL divergence between the new and the old policy must be lower than the
delta (δ), where delta is the size of the region.</strong> I could get into some math,
but I think that this will only complicate things rather than clarify them. So
essentially, we have just a <strong>constrained optimization problem.</strong></p>

<p><img src="/assets/img/posts/trpo_eq.jpg" alt="trpo" /></p>

<p>The question now is how we solve a constrained optimization problem? Using the
<a href="https://en.wikipedia.org/wiki/Conjugate_gradient_method">Conjugate Gradient
method</a>. We can, of
course, solve the problem analytically (natural gradient descent), but it is
computational ineffective. If you dust of your knowledge on numerical
mathematics, you might remember that the conjugate gradient method provides a
numeric solution to a system of equations. That is way better from a
computational perspective. So all we have to do is to approximate linearly the
objective function and quadratically the constraint and let the conjugate
gradient do its work.</p>

<p>To wrap it all up, the algorithm has the following steps:</p>

<ul>
  <li>We run a set of trajectories and collect the policies</li>
  <li>Estimate the advantages using advantage estimation algorithm</li>
  <li>Solve the constrained optimization problem using conjugate gradient</li>
  <li>Repeat</li>
</ul>

<p>Generally speaking, trust regions are considered pretty standard methods to
approach optimization problems. The tricky part is to apply them in a
reinforcement learning context in a way that provides an advantage over simple
policy gradients.</p>

<p>Although TRPO is a very powerful algorithm, it suffers from a significant
problem: that bloody constraint, which adds additional overhead to our
optimization problem. I mean it forces us to use the conjugate gradient method
and baffled us with linear and quadratic approximations. Wouldn’t it be nice if
the could somehow include the constraint directly into our optimization
objective? As you might have guessed that is exactly what Proximal policy
optimization does.</p>

<h2 id="proximal-policy-optimization-ppo">Proximal policy optimization (PPO)</h2>

<p>This simple idea gave us a quite simpler and more intuitive algorithm than TRPO.
And it turns out that it outperforms many of the existing techniques most of the
time. So instead of adding a constraint separately, we incorporate it inside the
objective function as a penalty (we subtract the KL divergence times a constant
C from the function).</p>

<p><img src="/assets/img/posts/ppo_penalty.jpg" alt="ppo_penalty" /></p>

<p>As soon as we do that there is no need to solve a constrained problem and we can
instead use a simple stochastic gradient descent in the above function. And
the algorithm is transformed as follows:</p>

<ul>
  <li>We run a set of trajectories and collect the policies</li>
  <li>Estimate the advantages using an advantage estimation algorithm</li>
  <li>Perform stochastic gradient descent on the objective function for a certain
number of epochs</li>
  <li>Repeat</li>
</ul>

<p>A small caveat is that is hard to choose the coefficient <em>C</em> in a way that it
works well over the whole course of optimization. To address that we update the
coefficient based on how big or small the KL divergence is. If KL is too high,
we increase it, or if it is too low, we decrease it.</p>

<p>So this is it? This is the famous proximal policy optimization? Actually no.
Sorry about that. It turns out that the function described above is not the same
as the original paper. The authors found a way to improve this penalized version
into a new, more robust objective function.</p>

<p><img src="/assets/img/posts/ppo.jpg" alt="ppo" /></p>

<p>Hey hey wait. What happened here? Actually, not much. Let me explain. One thing
I intentionally left out so far is that fraction of probabilities over there,
that seems to appear in TRPO also. Well, that is called importance sampling. We
essentially have a new policy that we want to estimate and an old one that we
use to collect the samples. With <strong>importance sampling</strong>, we can evaluate a new
policy with the samples from the old one and improve sample efficiency. The
ratio infers how different the two policies are and we denote is as r(theta).</p>

<p>Using this ratio, we can construct a new objective function to <strong>clip the
estimated advantage if the new policy is far away from the old one</strong>. And that’s
exactly what the above equation does. If an action is a lot more likely under
the new policy than the old one, we do not want to overdo the action update, so
we clipped the objective function. If it is much less likely under the new
policy than the old, the objective action is flattened out to prevent from
overdoing the update once again.</p>

<p>It may be just me, but I think I haven’t come across a simpler and cleaner
reinforcement learning algorithm in a while.</p>

<p>If you want to get into the rabbit hole, check out the baseline
<a href="https://github.com/openai/baselines/tree/master/baselines/ppo1">code</a> from
OpenAI, which will give you a crystal clear image of the whole algorithm and
solve all your questions. Of course, it would be much better if you try to
implement it from scratch all by yourself. It is not the easiest task, but why
not.</p>

<p>I think that’s it for now. Keep learning…</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Actor_critics/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            The idea behind Actor-Criti...
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Semantic_Segmentation/">
            Semantic Segmentation in th...
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="/lib/jquery/jquery.min.js"></script>
 <script src="/lib/jquery/jquery-migrate.min.js"></script>
 <script src="/lib/popper/popper.min.js"></script>
 <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="/lib/easing/easing.min.js"></script>
 <script src="/lib/counterup/jquery.waypoints.min.js"></script>
 <script src="/lib/counterup/jquery.counterup.js"></script>
 <script src="/lib/lightbox/js/lightbox.min.js"></script>
 <script src="/lib/typed/typed.min.js"></script>
 <script src="/js/scripts.js"></script>
 <script src="/js/contact_form.js"></script>



  </body>

</html>



