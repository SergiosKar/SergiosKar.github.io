
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Taking Deep Q Networks a step further | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Taking Deep Q Networks a step further" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Taking Deep Q Networks a step further Hello again, Today’s topic is … well, the same as the last one. Q Learning and Deep Q Networks. Last time, we explained what Q Learning is and how to use the Bellman equation to find the Q-values and as a result the optimal policy. Later, we introduced Deep Q Networks and how instead of computing all the values of the Q-table, we let a Deep Neural Network learn to approximate them. Deep Q Networks take as input the state of the environment and output a Q value for each possible action. The maximum Q value determines, which action the agent will perform. The training of the agents uses as loss the TD Error, which is the difference between the maximum possible value for the next state and the current prediction of the Q-value (as the Bellman equation suggests). As a result, we manage to approximate the Q-tables using a Neural Network. So far so good. But of course, there are a few problems that arise. It’s just the way scientific research is moving forward. And of course, we have come up with some great solutions. Moving Q-Targets The first problem is what is called moving Q-targets. As we saw, the first component of the TD Error (TD stands for Temporal Difference) is the Q-Target and it is calculated as the immediate reward plus the discounted max Q-value for the next state. When we train our agent, we update the weights accordingly to the TD Error. But the same weights apply to both the target and the predicted value. You see the problem? We move the output closer to the target, but we also move the target. So, we end up chasing the target and we get a highly oscillated training process. Wouldn’t be great to keep the target fixed as we train the network. Well, DeepMind did exactly that. Instead of using one Neural Network, it uses two. Yes, you heard that right! (like one wasn’t enough already). One as the main Deep Q Network and a second one (called Target Network) to update exclusively and periodically the weights of the target. This technique is called Fixed Q-Targets. In fact, the weights are fixed for the largest part of the training and they updated only once in a while. class DQNAgent: def __init__(self, state_size, action_size): self.model = self._build_model() self.target_model = self._build_model() self.update_target_model() def update_target_model(self): # copy weights from model to target_model self.target_model.set_weights(self.model.get_weights()) Maximization Bias Maximization bias is the tendency of Deep Q Networks to overestimate both the value and the action-value (Q) functions. Why does it happen? Think that if for some reason the network overestimates a Q value for an action, that action will be chosen as the go-to action for the next step and the same overestimated value will be used as a target value. In other words, there is no way to evaluate if the action with the max value is actually the best action. How to solve this? The answer is a very interesting method and is called: Double Deep Q Network To address maximization bias, we use two Deep Q Networks. On the one hand, the DQN is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action. The trick is that the target value is not automatically produced by the maximum Q-value, but by the Target network. In other words, we call forth the Target network to calculate the target Q value of taking that action at the next state. And as a side effect, we also solve the moving target problem. Neat right? Two birds with one stone. By decoupling the action selection from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. def train_model(self): if len(self.memory) &lt; self.train_start: return batch_size = min(self.batch_size, len(self.memory)) mini_batch = random.sample(self.memory, batch_size) update_input = np.zeros((batch_size, self.state_size)) update_target = np.zeros((batch_size, self.state_size)) action, reward, done = [], [], [] for i in range(batch_size): update_input[i] = mini_batch[i][0] action.append(mini_batch[i][1]) reward.append(mini_batch[i][2]) update_target[i] = mini_batch[i][3] done.append(mini_batch[i][4]) target = self.model.predict(update_input) target_next = self.model.predict(update_target) #DQN target_val = self.target_model.predict(update_target) #Target model for i in range(self.batch_size): if done[i]: target[i][action[i]] = reward[i] else: # selection of action is from model # update is from target model a = np.argmax(target_next[i]) target[i][action[i]] = reward[i] + self.discount_factor * (target_val[i][a]) self.model.fit(update_input, target, batch_size=self.batch_size,epochs=1, verbose=0) You think that’s it? Sorry to let you down. We are going to take this even further. What now? Are you going to add a third Neural Network? Haha!! Well kind of. Who’s laughing now? Dueling Deep Q Networks Let’s refresh the basis of Q Learning first. Q values correspond to a metric of how good an action is for a particular state right? That’s why it is an action-value function. The metric is nothing more the expected return of that action from the state. Q values can, in fact, be decomposed into two pieces: the state Value function V(s) and the advantage value A(s, a). And yes, we just introduce one more function: Q(s, a)= V(s)+ A(s,a) Advantage function captures how better an action is compared to the others at a given state, while as we know the value function captures how good it is to be at this state. The whole idea behind Dueling Q Networks relies on the representation of the Q function as a sum of the Value and the advantage function. We simply have two networks to learn each part of the sum and then we aggregate their outputs. Do we earn something by doing that? Of course, we do. The agents in now able to evaluate a state without caring about the effect of each action from that state. Meaning that the features that determined whether a state is good or nor are not necessarily the same as the features that evaluate an action. And it may not need to care for actions at all. It is not uncommon to have actions from a state that do not affect the environment at all. So why take them into consideration? * Quick note: If you take a closer look at the image, you will see that to aggregate the output of the two networks we do not simply add them. The reason behind that is the issue of identifiability. If we have the Q, we can’t find the V and A. So, we can’t back propagate. Instead, we choose to use the mean advantage as a baseline (the subtracted term). def build_model(self): input = Input(shape=self.state_size) shared = Conv2D(32, (8, 8), strides=(4, 4), activation=&#39;relu&#39;)(input) shared = Conv2D(64, (4, 4), strides=(2, 2), activation=&#39;relu&#39;)(shared) shared = Conv2D(64, (3, 3), strides=(1, 1), activation=&#39;relu&#39;)(shared) flatten = Flatten()(shared) # network separate state value and advantages advantage_fc = Dense(512, activation=&#39;relu&#39;)(flatten) advantage = Dense(self.action_size)(advantage_fc) advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(self.action_size,))(advantage) value_fc = Dense(512, activation=&#39;relu&#39;)(flatten) value = Dense(1)(value_fc) value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(self.action_size,))(value) q_value = merge([value, advantage], mode=&#39;sum&#39;) model = Model(inputs=input, outputs=q_value) model.summary() return model Last but not least, we have on more issue to discuss and it has to do with optimizing the experience replay. Prioritized Experience Replay Do you remember that experience replay is when we replay to the agent random past experiences every now and then to prevent him from forgetting them? If you don’t, now you do. But some experiences may be more significant than others. As a result, we should prioritize them to be replayed. To do just that, instead of sampling randomly (from a uniform distribution), we sample using a priority. As priority we define the magnitude of the TD Error (plus some constant to avoid zero probability for an experience to be chosen). p = |δ|+ε Central idea: The highest the error between the prediction and the target, the more urgent is to learn about it. And to ensure that we won’t always replay the same experience, we add some stochasticity and we are all set. Also, for complexity’s shake we save the experiences in a binary tree called SumTree. from SumTree import SumTree class PER: # stored as ( s, a, r, s_ ) in SumTree e = 0.01 a = 0.6 def __init__(self, capacity): self.tree = SumTree(capacity) def _getPriority(self, error): return (error + self.e) ** self.a def add(self, error, sample): p = self._getPriority(error) self.tree.add(p, sample) def sample(self, n): batch = [] segment = self.tree.total() / n for i in range(n): a = segment * i b = segment * (i + 1) s = random.uniform(a, b) (idx, p, data) = self.tree.get(s) batch.append((idx, data)) return batch def update(self, idx, error): p = self._getPriority(error) self.tree.update(idx, p) That was a lot. A lot of new information, a lot of new improvements. But just think what we can combine all of them together. And we do it. I tried to give a summary of the most important recent efforts in the field, backed by some intuitive thought and some math. This is why Reinforcement Learning is so important to learn. There is so much potential and so many capabilities for enhancements that you just can’t ignore the fact that is going to be the big player in AI (if it already isn’t). But that’s why is so hard to learn and to keep up with it. All we have to do is keep learning …" />
<meta property="og:description" content="Taking Deep Q Networks a step further Hello again, Today’s topic is … well, the same as the last one. Q Learning and Deep Q Networks. Last time, we explained what Q Learning is and how to use the Bellman equation to find the Q-values and as a result the optimal policy. Later, we introduced Deep Q Networks and how instead of computing all the values of the Q-table, we let a Deep Neural Network learn to approximate them. Deep Q Networks take as input the state of the environment and output a Q value for each possible action. The maximum Q value determines, which action the agent will perform. The training of the agents uses as loss the TD Error, which is the difference between the maximum possible value for the next state and the current prediction of the Q-value (as the Bellman equation suggests). As a result, we manage to approximate the Q-tables using a Neural Network. So far so good. But of course, there are a few problems that arise. It’s just the way scientific research is moving forward. And of course, we have come up with some great solutions. Moving Q-Targets The first problem is what is called moving Q-targets. As we saw, the first component of the TD Error (TD stands for Temporal Difference) is the Q-Target and it is calculated as the immediate reward plus the discounted max Q-value for the next state. When we train our agent, we update the weights accordingly to the TD Error. But the same weights apply to both the target and the predicted value. You see the problem? We move the output closer to the target, but we also move the target. So, we end up chasing the target and we get a highly oscillated training process. Wouldn’t be great to keep the target fixed as we train the network. Well, DeepMind did exactly that. Instead of using one Neural Network, it uses two. Yes, you heard that right! (like one wasn’t enough already). One as the main Deep Q Network and a second one (called Target Network) to update exclusively and periodically the weights of the target. This technique is called Fixed Q-Targets. In fact, the weights are fixed for the largest part of the training and they updated only once in a while. class DQNAgent: def __init__(self, state_size, action_size): self.model = self._build_model() self.target_model = self._build_model() self.update_target_model() def update_target_model(self): # copy weights from model to target_model self.target_model.set_weights(self.model.get_weights()) Maximization Bias Maximization bias is the tendency of Deep Q Networks to overestimate both the value and the action-value (Q) functions. Why does it happen? Think that if for some reason the network overestimates a Q value for an action, that action will be chosen as the go-to action for the next step and the same overestimated value will be used as a target value. In other words, there is no way to evaluate if the action with the max value is actually the best action. How to solve this? The answer is a very interesting method and is called: Double Deep Q Network To address maximization bias, we use two Deep Q Networks. On the one hand, the DQN is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action. The trick is that the target value is not automatically produced by the maximum Q-value, but by the Target network. In other words, we call forth the Target network to calculate the target Q value of taking that action at the next state. And as a side effect, we also solve the moving target problem. Neat right? Two birds with one stone. By decoupling the action selection from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. def train_model(self): if len(self.memory) &lt; self.train_start: return batch_size = min(self.batch_size, len(self.memory)) mini_batch = random.sample(self.memory, batch_size) update_input = np.zeros((batch_size, self.state_size)) update_target = np.zeros((batch_size, self.state_size)) action, reward, done = [], [], [] for i in range(batch_size): update_input[i] = mini_batch[i][0] action.append(mini_batch[i][1]) reward.append(mini_batch[i][2]) update_target[i] = mini_batch[i][3] done.append(mini_batch[i][4]) target = self.model.predict(update_input) target_next = self.model.predict(update_target) #DQN target_val = self.target_model.predict(update_target) #Target model for i in range(self.batch_size): if done[i]: target[i][action[i]] = reward[i] else: # selection of action is from model # update is from target model a = np.argmax(target_next[i]) target[i][action[i]] = reward[i] + self.discount_factor * (target_val[i][a]) self.model.fit(update_input, target, batch_size=self.batch_size,epochs=1, verbose=0) You think that’s it? Sorry to let you down. We are going to take this even further. What now? Are you going to add a third Neural Network? Haha!! Well kind of. Who’s laughing now? Dueling Deep Q Networks Let’s refresh the basis of Q Learning first. Q values correspond to a metric of how good an action is for a particular state right? That’s why it is an action-value function. The metric is nothing more the expected return of that action from the state. Q values can, in fact, be decomposed into two pieces: the state Value function V(s) and the advantage value A(s, a). And yes, we just introduce one more function: Q(s, a)= V(s)+ A(s,a) Advantage function captures how better an action is compared to the others at a given state, while as we know the value function captures how good it is to be at this state. The whole idea behind Dueling Q Networks relies on the representation of the Q function as a sum of the Value and the advantage function. We simply have two networks to learn each part of the sum and then we aggregate their outputs. Do we earn something by doing that? Of course, we do. The agents in now able to evaluate a state without caring about the effect of each action from that state. Meaning that the features that determined whether a state is good or nor are not necessarily the same as the features that evaluate an action. And it may not need to care for actions at all. It is not uncommon to have actions from a state that do not affect the environment at all. So why take them into consideration? * Quick note: If you take a closer look at the image, you will see that to aggregate the output of the two networks we do not simply add them. The reason behind that is the issue of identifiability. If we have the Q, we can’t find the V and A. So, we can’t back propagate. Instead, we choose to use the mean advantage as a baseline (the subtracted term). def build_model(self): input = Input(shape=self.state_size) shared = Conv2D(32, (8, 8), strides=(4, 4), activation=&#39;relu&#39;)(input) shared = Conv2D(64, (4, 4), strides=(2, 2), activation=&#39;relu&#39;)(shared) shared = Conv2D(64, (3, 3), strides=(1, 1), activation=&#39;relu&#39;)(shared) flatten = Flatten()(shared) # network separate state value and advantages advantage_fc = Dense(512, activation=&#39;relu&#39;)(flatten) advantage = Dense(self.action_size)(advantage_fc) advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(self.action_size,))(advantage) value_fc = Dense(512, activation=&#39;relu&#39;)(flatten) value = Dense(1)(value_fc) value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(self.action_size,))(value) q_value = merge([value, advantage], mode=&#39;sum&#39;) model = Model(inputs=input, outputs=q_value) model.summary() return model Last but not least, we have on more issue to discuss and it has to do with optimizing the experience replay. Prioritized Experience Replay Do you remember that experience replay is when we replay to the agent random past experiences every now and then to prevent him from forgetting them? If you don’t, now you do. But some experiences may be more significant than others. As a result, we should prioritize them to be replayed. To do just that, instead of sampling randomly (from a uniform distribution), we sample using a priority. As priority we define the magnitude of the TD Error (plus some constant to avoid zero probability for an experience to be chosen). p = |δ|+ε Central idea: The highest the error between the prediction and the target, the more urgent is to learn about it. And to ensure that we won’t always replay the same experience, we add some stochasticity and we are all set. Also, for complexity’s shake we save the experiences in a binary tree called SumTree. from SumTree import SumTree class PER: # stored as ( s, a, r, s_ ) in SumTree e = 0.01 a = 0.6 def __init__(self, capacity): self.tree = SumTree(capacity) def _getPriority(self, error): return (error + self.e) ** self.a def add(self, error, sample): p = self._getPriority(error) self.tree.add(p, sample) def sample(self, n): batch = [] segment = self.tree.total() / n for i in range(n): a = segment * i b = segment * (i + 1) s = random.uniform(a, b) (idx, p, data) = self.tree.get(s) batch.append((idx, data)) return batch def update(self, idx, error): p = self._getPriority(error) self.tree.update(idx, p) That was a lot. A lot of new information, a lot of new improvements. But just think what we can combine all of them together. And we do it. I tried to give a summary of the most important recent efforts in the field, backed by some intuitive thought and some math. This is why Reinforcement Learning is so important to learn. There is so much potential and so many capabilities for enhancements that you just can’t ignore the fact that is going to be the big player in AI (if it already isn’t). But that’s why is so hard to learn and to keep up with it. All we have to do is keep learning …" />
<link rel="canonical" href="//taking_deep_q_networks_a_step_further/" />
<meta property="og:url" content="//taking_deep_q_networks_a_step_further/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-14T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"//taking_deep_q_networks_a_step_further/","headline":"Taking Deep Q Networks a step further","dateModified":"2018-10-14T00:00:00+03:00","datePublished":"2018-10-14T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"//taking_deep_q_networks_a_step_further/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"Taking Deep Q Networks a step further Hello again, Today’s topic is … well, the same as the last one. Q Learning and Deep Q Networks. Last time, we explained what Q Learning is and how to use the Bellman equation to find the Q-values and as a result the optimal policy. Later, we introduced Deep Q Networks and how instead of computing all the values of the Q-table, we let a Deep Neural Network learn to approximate them. Deep Q Networks take as input the state of the environment and output a Q value for each possible action. The maximum Q value determines, which action the agent will perform. The training of the agents uses as loss the TD Error, which is the difference between the maximum possible value for the next state and the current prediction of the Q-value (as the Bellman equation suggests). As a result, we manage to approximate the Q-tables using a Neural Network. So far so good. But of course, there are a few problems that arise. It’s just the way scientific research is moving forward. And of course, we have come up with some great solutions. Moving Q-Targets The first problem is what is called moving Q-targets. As we saw, the first component of the TD Error (TD stands for Temporal Difference) is the Q-Target and it is calculated as the immediate reward plus the discounted max Q-value for the next state. When we train our agent, we update the weights accordingly to the TD Error. But the same weights apply to both the target and the predicted value. You see the problem? We move the output closer to the target, but we also move the target. So, we end up chasing the target and we get a highly oscillated training process. Wouldn’t be great to keep the target fixed as we train the network. Well, DeepMind did exactly that. Instead of using one Neural Network, it uses two. Yes, you heard that right! (like one wasn’t enough already). One as the main Deep Q Network and a second one (called Target Network) to update exclusively and periodically the weights of the target. This technique is called Fixed Q-Targets. In fact, the weights are fixed for the largest part of the training and they updated only once in a while. class DQNAgent: def __init__(self, state_size, action_size): self.model = self._build_model() self.target_model = self._build_model() self.update_target_model() def update_target_model(self): # copy weights from model to target_model self.target_model.set_weights(self.model.get_weights()) Maximization Bias Maximization bias is the tendency of Deep Q Networks to overestimate both the value and the action-value (Q) functions. Why does it happen? Think that if for some reason the network overestimates a Q value for an action, that action will be chosen as the go-to action for the next step and the same overestimated value will be used as a target value. In other words, there is no way to evaluate if the action with the max value is actually the best action. How to solve this? The answer is a very interesting method and is called: Double Deep Q Network To address maximization bias, we use two Deep Q Networks. On the one hand, the DQN is responsible for the selection of the next action (the one with the maximum value) as always. On the other hand, the Target network is responsible for the evaluation of that action. The trick is that the target value is not automatically produced by the maximum Q-value, but by the Target network. In other words, we call forth the Target network to calculate the target Q value of taking that action at the next state. And as a side effect, we also solve the moving target problem. Neat right? Two birds with one stone. By decoupling the action selection from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. def train_model(self): if len(self.memory) &lt; self.train_start: return batch_size = min(self.batch_size, len(self.memory)) mini_batch = random.sample(self.memory, batch_size) update_input = np.zeros((batch_size, self.state_size)) update_target = np.zeros((batch_size, self.state_size)) action, reward, done = [], [], [] for i in range(batch_size): update_input[i] = mini_batch[i][0] action.append(mini_batch[i][1]) reward.append(mini_batch[i][2]) update_target[i] = mini_batch[i][3] done.append(mini_batch[i][4]) target = self.model.predict(update_input) target_next = self.model.predict(update_target) #DQN target_val = self.target_model.predict(update_target) #Target model for i in range(self.batch_size): if done[i]: target[i][action[i]] = reward[i] else: # selection of action is from model # update is from target model a = np.argmax(target_next[i]) target[i][action[i]] = reward[i] + self.discount_factor * (target_val[i][a]) self.model.fit(update_input, target, batch_size=self.batch_size,epochs=1, verbose=0) You think that’s it? Sorry to let you down. We are going to take this even further. What now? Are you going to add a third Neural Network? Haha!! Well kind of. Who’s laughing now? Dueling Deep Q Networks Let’s refresh the basis of Q Learning first. Q values correspond to a metric of how good an action is for a particular state right? That’s why it is an action-value function. The metric is nothing more the expected return of that action from the state. Q values can, in fact, be decomposed into two pieces: the state Value function V(s) and the advantage value A(s, a). And yes, we just introduce one more function: Q(s, a)= V(s)+ A(s,a) Advantage function captures how better an action is compared to the others at a given state, while as we know the value function captures how good it is to be at this state. The whole idea behind Dueling Q Networks relies on the representation of the Q function as a sum of the Value and the advantage function. We simply have two networks to learn each part of the sum and then we aggregate their outputs. Do we earn something by doing that? Of course, we do. The agents in now able to evaluate a state without caring about the effect of each action from that state. Meaning that the features that determined whether a state is good or nor are not necessarily the same as the features that evaluate an action. And it may not need to care for actions at all. It is not uncommon to have actions from a state that do not affect the environment at all. So why take them into consideration? * Quick note: If you take a closer look at the image, you will see that to aggregate the output of the two networks we do not simply add them. The reason behind that is the issue of identifiability. If we have the Q, we can’t find the V and A. So, we can’t back propagate. Instead, we choose to use the mean advantage as a baseline (the subtracted term). def build_model(self): input = Input(shape=self.state_size) shared = Conv2D(32, (8, 8), strides=(4, 4), activation=&#39;relu&#39;)(input) shared = Conv2D(64, (4, 4), strides=(2, 2), activation=&#39;relu&#39;)(shared) shared = Conv2D(64, (3, 3), strides=(1, 1), activation=&#39;relu&#39;)(shared) flatten = Flatten()(shared) # network separate state value and advantages advantage_fc = Dense(512, activation=&#39;relu&#39;)(flatten) advantage = Dense(self.action_size)(advantage_fc) advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(self.action_size,))(advantage) value_fc = Dense(512, activation=&#39;relu&#39;)(flatten) value = Dense(1)(value_fc) value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(self.action_size,))(value) q_value = merge([value, advantage], mode=&#39;sum&#39;) model = Model(inputs=input, outputs=q_value) model.summary() return model Last but not least, we have on more issue to discuss and it has to do with optimizing the experience replay. Prioritized Experience Replay Do you remember that experience replay is when we replay to the agent random past experiences every now and then to prevent him from forgetting them? If you don’t, now you do. But some experiences may be more significant than others. As a result, we should prioritize them to be replayed. To do just that, instead of sampling randomly (from a uniform distribution), we sample using a priority. As priority we define the magnitude of the TD Error (plus some constant to avoid zero probability for an experience to be chosen). p = |δ|+ε Central idea: The highest the error between the prediction and the target, the more urgent is to learn about it. And to ensure that we won’t always replay the same experience, we add some stochasticity and we are all set. Also, for complexity’s shake we save the experiences in a binary tree called SumTree. from SumTree import SumTree class PER: # stored as ( s, a, r, s_ ) in SumTree e = 0.01 a = 0.6 def __init__(self, capacity): self.tree = SumTree(capacity) def _getPriority(self, error): return (error + self.e) ** self.a def add(self, error, sample): p = self._getPriority(error) self.tree.add(p, sample) def sample(self, n): batch = [] segment = self.tree.total() / n for i in range(n): a = segment * i b = segment * (i + 1) s = random.uniform(a, b) (idx, p, data) = self.tree.get(s) batch.append((idx, data)) return batch def update(self, idx, error): p = self._getPriority(error) self.tree.update(idx, p) That was a lot. A lot of new information, a lot of new improvements. But just think what we can combine all of them together. And we do it. I tried to give a summary of the most important recent efforts in the field, backed by some intuitive thought and some math. This is why Reinforcement Learning is so important to learn. There is so much potential and so many capabilities for enhancements that you just can’t ignore the fact that is going to be the big player in AI (if it already isn’t). But that’s why is so hard to learn and to keep up with it. All we have to do is keep learning …","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href="/"> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( //assets/img/posts/DDQN.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">Taking Deep Q Networks a step further</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp Oct 14, 2018</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            12 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="taking-deep-q-networks-a-step-further">Taking Deep Q Networks a step further</h1>

<p>Hello again,</p>

<p>Today’s topic is … well, the same as the last one. Q Learning and Deep Q
Networks. <a href="https://sergioskar.github.io/Deep_Q_Learning/">Last time</a>, we explained what Q Learning is and how to use the Bellman
equation to find the Q-values and as a result the optimal policy. Later, we
introduced Deep Q Networks and how instead of computing all the values of the
Q-table, we let a Deep Neural Network learn to approximate them.</p>

<p>Deep Q Networks take as input the state of the environment and output a Q value
for each possible action. The maximum Q value determines, which action the agent
will perform. The training of the agents uses as loss the <strong>TD Error</strong>, which is
the difference between the maximum possible value for the next state and the
current prediction of the Q-value (as the Bellman equation suggests). As a
result, we manage to approximate the Q-tables using a Neural Network.</p>

<p>So far so good. But of course, there are a few problems that arise. It’s just
the way scientific research is moving forward. And of course, we have come up
with some great solutions.</p>

<h2 id="moving-q-targets">Moving Q-Targets</h2>

<p>The first problem is what is called moving Q-targets. As we saw, the first
component of the TD Error (TD stands for Temporal Difference) is the Q-Target
and it is calculated as the immediate reward plus the discounted max Q-value for
the next state. When we train our agent, we update the weights accordingly to
the TD Error. But the same weights apply to both the target and the predicted
value. You see the problem?</p>

<p><img src="//assets/img/posts/TDError.jpg" alt="TDError" /></p>

<p>We move the output closer to the target, but we also move the target. So, we end
up chasing the target and we get a highly oscillated training process. Wouldn’t
be great to keep the target fixed as we train the network. Well, DeepMind did
exactly that.</p>

<p>Instead of using one Neural Network, it uses two. Yes, you heard that right!
(like one wasn’t enough already).</p>

<p>One as the main Deep Q Network and a second one (called <strong>Target Network</strong>) to
update exclusively and periodically the weights of the target. This technique is
called <strong>Fixed Q-Targets.</strong> In fact, the weights are fixed for the largest part
of the training and they updated only once in a while.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_build_model</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_target_model</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_target_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># copy weights from model to target_model
</span>        <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">())</span>

</code></pre></div></div>

<h2 id="maximization-bias">Maximization Bias</h2>

<p>Maximization bias is the tendency of Deep Q Networks to overestimate both the
value and the action-value (Q) functions. Why does it happen? Think that if for
some reason the network overestimates a Q value for an action, that action will
be chosen as the go-to action for the next step and the same overestimated value
will be used as a target value. In other words, there is no way to evaluate if
the action with the max value is actually the best action. How to solve this?
The answer is a very interesting method and is called:</p>

<h2 id="double-deep-q-network">Double Deep Q Network</h2>

<p>To address maximization bias, we use two Deep Q Networks.</p>

<ul>
  <li>
    <p>On the one hand, the DQN is responsible for the <strong>selection</strong> of the next
action (the one with the maximum value) as always.</p>
  </li>
  <li>
    <p>On the other hand, the Target network is responsible for the <strong>evaluation</strong>
of that action.</p>
  </li>
</ul>

<p>The trick is that the target value is not automatically produced by the maximum
Q-value, but by the Target network. In other words, we call forth the Target
network to calculate the target Q value of taking that action at the next state.
And as a side effect, we also solve the moving target problem. Neat right? Two
birds with one stone. By decoupling the action selection from the target Q-value
generation, we are able to substantially reduce the overestimation, and train
faster and more reliably.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_start</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">))</span>
        <span class="n">mini_batch</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">update_input</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
        <span class="n">update_target</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
        <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[],</span> <span class="p">[]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">update_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">action</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">reward</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">update_target</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">mini_batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">done</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mini_batch</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">4</span><span class="p">])</span>

        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">update_input</span><span class="p">)</span>
        <span class="n">target_next</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">update_target</span><span class="p">)</span> <span class="c1">#DQN
</span>        <span class="n">target_val</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">target_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">update_target</span><span class="p">)</span> <span class="c1">#Target model
</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
                <span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">reward</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># selection of action is from model 
</span>                <span class="c1"># update is from target model
</span>                <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">target_next</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
                <span class="n">target</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">action</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="n">reward</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">discount_factor</span> <span class="o">*</span> <span class="p">(</span><span class="n">target_val</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">a</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">update_input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>You think that’s it? Sorry to let you down. We are going to take this even
further. What now? Are you going to add a third Neural Network? Haha!!</p>

<p>Well kind of. Who’s laughing now?</p>

<h2 id="dueling-deep-q-networks">Dueling Deep Q Networks</h2>

<p>Let’s refresh the basis of Q Learning first. Q values correspond to a metric of
how good an action is for a particular state right? That’s why it is an
action-value function. The metric is nothing more the expected return of that
action from the state. Q values can, in fact, be decomposed into two pieces: the
state Value function V(s) and the advantage value A(s, a). And yes, we just
introduce one more function:</p>

<p>Q(s, a)= V(s)+ A(s,a)</p>

<p>Advantage function captures how better an action is compared to the others at a
given state, while as we know the value function captures how good it is to be
at this state. The whole idea behind Dueling Q Networks relies on <strong>the
representation of the Q function as a sum of the Value and the advantage
function</strong>. We simply have two networks to learn each part of the sum and then
we aggregate their outputs.</p>

<p><img src="//assets/img/posts/DDQN.jpg" alt="DDQN" /></p>

<p>Do we earn something by doing that? Of course, we do. The agents in now able to
evaluate a state without caring about the effect of each action from that state.
Meaning that the features that determined whether a state is good or nor are not
necessarily the same as the features that evaluate an action. And it may not
need to care for actions at all. It is not uncommon to have actions from a state
that do not affect the environment at all. So why take them into consideration?</p>

<p>* Quick note: If you take a closer look at the image, you will see that to
aggregate the output of the two networks we do not simply add them. The reason
behind that is the issue of identifiability. If we have the Q, we can’t find the
V and A. So, we can’t back propagate. Instead, we choose to use the mean
advantage as a baseline (the subtracted term).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">state_size</span><span class="p">)</span>
        <span class="n">shared</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="nb">input</span><span class="p">)</span>
        <span class="n">shared</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">shared</span><span class="p">)</span>
        <span class="n">shared</span> <span class="o">=</span> <span class="n">Conv2D</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">strides</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">shared</span><span class="p">)</span>
        <span class="n">flatten</span> <span class="o">=</span> <span class="n">Flatten</span><span class="p">()(</span><span class="n">shared</span><span class="p">)</span>

        <span class="c1"># network separate state value and advantages
</span>        <span class="n">advantage_fc</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">flatten</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">)(</span><span class="n">advantage_fc</span><span class="p">)</span>
        <span class="n">advantage</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">:</span> <span class="n">a</span><span class="p">[:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">K</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">[:,</span> <span class="p">:],</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
                           <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">,))(</span><span class="n">advantage</span><span class="p">)</span>

        <span class="n">value_fc</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">flatten</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span>  <span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">value_fc</span><span class="p">)</span>
        <span class="n">value</span> <span class="o">=</span> <span class="n">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">K</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">s</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
                       <span class="n">output_shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action_size</span><span class="p">,))(</span><span class="n">value</span><span class="p">)</span>

        <span class="n">q_value</span> <span class="o">=</span> <span class="n">merge</span><span class="p">([</span><span class="n">value</span><span class="p">,</span> <span class="n">advantage</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s">'sum'</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">q_value</span><span class="p">)</span>
        <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">model</span>

</code></pre></div></div>

<p>Last but not least, we have on more issue to discuss and it has to do with
optimizing the experience replay.</p>

<h2 id="prioritized-experience-replay">Prioritized Experience Replay</h2>

<p>Do you remember that experience replay is when we replay to the agent random
past experiences every now and then to prevent him from forgetting them? If you
don’t, now you do. But some experiences may be more significant than others. As
a result, we should prioritize them to be replayed. To do just that, instead of
sampling randomly (from a uniform distribution), we sample using a priority. As
priority we define the magnitude of the TD Error (plus some constant to avoid
zero probability for an experience to be chosen).</p>

<blockquote>
  <p>p = |δ|+ε</p>
</blockquote>

<p>Central idea: <strong>The highest the error between the prediction and the target, the
more urgent is to learn about it</strong>.</p>

<p>And to ensure that we won’t always replay the same experience, we add some
stochasticity and we are all set. Also, for complexity’s shake we save the experiences in 
a binary tree called SumTree.</p>

<p><img src="//assets/img/posts/PER.jpg" alt="PER" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">SumTree</span> <span class="kn">import</span> <span class="n">SumTree</span>

<span class="k">class</span> <span class="nc">PER</span><span class="p">:</span>  <span class="c1"># stored as ( s, a, r, s_ ) in SumTree
</span>    <span class="n">e</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">a</span> <span class="o">=</span> <span class="mf">0.6</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">capacity</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span> <span class="o">=</span> <span class="n">SumTree</span><span class="p">(</span><span class="n">capacity</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_getPriority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">error</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">e</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">a</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">error</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_getPriority</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">sample</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">segment</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">total</span><span class="p">()</span> <span class="o">/</span> <span class="n">n</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">segment</span> <span class="o">*</span> <span class="n">i</span>
            <span class="n">b</span> <span class="o">=</span> <span class="n">segment</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">s</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
            <span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">batch</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">idx</span><span class="p">,</span> <span class="n">data</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">batch</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">error</span><span class="p">):</span>
        <span class="n">p</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_getPriority</span><span class="p">(</span><span class="n">error</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></div>

<p>That was a lot. A lot of new information, a lot of new improvements. But just
think what we can combine all of them together. And we do it.</p>

<p>I tried to give a summary of the most important recent efforts in the field,
backed by some intuitive thought and some math. This is why Reinforcement
Learning is so important to learn. There is so much potential and so many
capabilities for enhancements that you just can’t ignore the fact that is going
to be the big player in AI (if it already isn’t). But that’s why is so hard to
learn and to keep up with it.</p>

<p>All we have to do is keep learning …</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Deep_Q_Learning/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            Deep Q Learning
            </a>
            
        </div>

        <div class="controls__item next">
            
            <span>Next</span>
            <a href="/Policy-Gradients/">
            Policy Gradients
            <span>
                <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="#fillColor" d="M.353 9.282c-.37.434-.323 1.09.106 1.465a1.016 1.016 0 0 0 1.446-.107L5.75 6.125a1.05 1.05 0 0 0-.017-1.378L1.784.34A1.015 1.015 0 0 0 .336.27a1.05 1.05 0 0 0-.07 1.468l3.34 3.725L.353 9.282z"/>
                </svg>
                </span>
            </a>
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="//lib/jquery/jquery.min.js"></script>
 <script src="//lib/jquery/jquery-migrate.min.js"></script>
 <script src="//lib/popper/popper.min.js"></script>
 <script src="//lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="//lib/easing/easing.min.js"></script>
 <script src="//lib/counterup/jquery.waypoints.min.js"></script>
 <script src="//lib/counterup/jquery.counterup.js"></script>
 <script src="//lib/lightbox/js/lightbox.min.js"></script>
 <script src="//lib/typed/typed.min.js"></script>
 <script src="//js/scripts.js"></script>
 <script src="//js/contact_form.js"></script>



  </body>

</html>



