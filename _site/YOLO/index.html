
<!DOCTYPE html>
<html lang="en-US">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Begin Jekyll SEO tag v2.6.1 -->
<title>YOLO - You only look once | Sergios Karagiannakos</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="YOLO - You only look once" />
<meta name="author" content="Sergios Karagiannakos" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="YOLO - You only look once YOLO!!! So do we only live once? I sure do not know. What I know is that we only have to LOOK once. Wait what? That’s right. If you want to detect and localize objects in an image, there is no need to go through the whole process of proposing regions of interest, classify them and correct their bounding boxes. If you recall from my previous article, this is exactly what models like RCNN and Faster RCNN do. Do we really need all that complexity and computation? Well if we want top-notch accuracy we certainly do. Luckily there is another simpler way to perform such a task, by processing the image only once and output the prediction immediately. These types of models are called Single shot detectors. https://www.youtube.com/watch?v=yQwfDxBMtXg Single shot detectors Instead of having a dedicated system to propose regions of interests, we have a set of predefined boxes to look for objects, which are forwarded to a bunch of convolutional layers to predict class scores and bounding box offsets. Then for each box we predict a number of bounding boxes with a confidence score assigned to each one, we detect one object centered in that box and we output a set of probabilities for each possible class. Once we have all that, we simply and maybe naively keep only the box with a high confidence score. And it works. With very impressive results actually. To elaborate the overall flow even better, let’s use one of the most popular single shot detectors called YOLO . You only look once (YOLO) There have been 3 versions of the model so far, with each new one improving the previous in terms of both speed and accuracy. The number of predefined cells and the number of predicted bounding boxes for each cell is defined based on the input size and the classes. In our case, we are going to use the actual numbers used to evaluate the PASCAL VOC dataset. First, we divide the image into a grid of 13x13, resulting in 169 cells in total. For every one of the cells, it predicts 5 bounding boxes (x,y,w,h) with a confidence score, it detects one object regardless the number of boxes and 20 probabilities for the 20 classes. In total, we have 169*5=845 bounding boxes and the shape of output tensor of the mode is going to be (13,13,5*5+20)= (13,13,45). The whole essence of the YOLO models is to build this (13,13,45) tensor. To accomplish that, it uses a CNN network and 2 fully connected layers to perform the actual regression. The final prediction is extracted after keeping only the bounding boxes with a high confidence score( higher than a threshold such as 0.3) https://pjreddie.com/darknet/yolo/ Because the model may output duplicate detections for the same object, we use a technique called Non-maximal suppression to remove duplicates. In a simple implementation, we sort the predictions by the confidence score and as we iterate them we keep only the first appearances of each class. As far as the actual model is concerned, the architecture is quite trivial as it consists of only convolutional and pooling layers, without any fancy tricks. We train the model using a multiple loss function, which includes a classification loss, a localization loss and a confidence loss. The most recent versions of YOLO have introduced some special tricks to improve the accuracy and reduce the training and inference time. Some examples are batch normalization, anchor boxes, dimensions clusters and others. If you want to get into more details, you should definitely check the original papers. Also to dive into code and try the YOLO models in practice, check out these two awesome repositories in Github (repo1 and repo2). The power of YOLO is not its spectacular accuracy or the very clever ideas behind it, is its superb speed, which makes it ideal for embedded systems and low-power applications. That’s why self-driving cars and surveillance cameras are its most common real-world use cases. As deep learning continues to play along with computer vision (and it will sure do), we can expect many more models to be tailored for low-power systems even if they sometimes sacrify accuracy. And dont forget the whole Internet of Things kind of thing. This is where these models really shine." />
<meta property="og:description" content="YOLO - You only look once YOLO!!! So do we only live once? I sure do not know. What I know is that we only have to LOOK once. Wait what? That’s right. If you want to detect and localize objects in an image, there is no need to go through the whole process of proposing regions of interest, classify them and correct their bounding boxes. If you recall from my previous article, this is exactly what models like RCNN and Faster RCNN do. Do we really need all that complexity and computation? Well if we want top-notch accuracy we certainly do. Luckily there is another simpler way to perform such a task, by processing the image only once and output the prediction immediately. These types of models are called Single shot detectors. https://www.youtube.com/watch?v=yQwfDxBMtXg Single shot detectors Instead of having a dedicated system to propose regions of interests, we have a set of predefined boxes to look for objects, which are forwarded to a bunch of convolutional layers to predict class scores and bounding box offsets. Then for each box we predict a number of bounding boxes with a confidence score assigned to each one, we detect one object centered in that box and we output a set of probabilities for each possible class. Once we have all that, we simply and maybe naively keep only the box with a high confidence score. And it works. With very impressive results actually. To elaborate the overall flow even better, let’s use one of the most popular single shot detectors called YOLO . You only look once (YOLO) There have been 3 versions of the model so far, with each new one improving the previous in terms of both speed and accuracy. The number of predefined cells and the number of predicted bounding boxes for each cell is defined based on the input size and the classes. In our case, we are going to use the actual numbers used to evaluate the PASCAL VOC dataset. First, we divide the image into a grid of 13x13, resulting in 169 cells in total. For every one of the cells, it predicts 5 bounding boxes (x,y,w,h) with a confidence score, it detects one object regardless the number of boxes and 20 probabilities for the 20 classes. In total, we have 169*5=845 bounding boxes and the shape of output tensor of the mode is going to be (13,13,5*5+20)= (13,13,45). The whole essence of the YOLO models is to build this (13,13,45) tensor. To accomplish that, it uses a CNN network and 2 fully connected layers to perform the actual regression. The final prediction is extracted after keeping only the bounding boxes with a high confidence score( higher than a threshold such as 0.3) https://pjreddie.com/darknet/yolo/ Because the model may output duplicate detections for the same object, we use a technique called Non-maximal suppression to remove duplicates. In a simple implementation, we sort the predictions by the confidence score and as we iterate them we keep only the first appearances of each class. As far as the actual model is concerned, the architecture is quite trivial as it consists of only convolutional and pooling layers, without any fancy tricks. We train the model using a multiple loss function, which includes a classification loss, a localization loss and a confidence loss. The most recent versions of YOLO have introduced some special tricks to improve the accuracy and reduce the training and inference time. Some examples are batch normalization, anchor boxes, dimensions clusters and others. If you want to get into more details, you should definitely check the original papers. Also to dive into code and try the YOLO models in practice, check out these two awesome repositories in Github (repo1 and repo2). The power of YOLO is not its spectacular accuracy or the very clever ideas behind it, is its superb speed, which makes it ideal for embedded systems and low-power applications. That’s why self-driving cars and surveillance cameras are its most common real-world use cases. As deep learning continues to play along with computer vision (and it will sure do), we can expect many more models to be tailored for low-power systems even if they sometimes sacrify accuracy. And dont forget the whole Internet of Things kind of thing. This is where these models really shine." />
<link rel="canonical" href="/YOLO/" />
<meta property="og:url" content="/YOLO/" />
<meta property="og:site_name" content="Sergios Karagiannakos" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-05-13T00:00:00+03:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"/YOLO/","headline":"YOLO - You only look once","dateModified":"2019-05-13T00:00:00+03:00","datePublished":"2019-05-13T00:00:00+03:00","mainEntityOfPage":{"@type":"WebPage","@id":"/YOLO/"},"author":{"@type":"Person","name":"Sergios Karagiannakos"},"description":"YOLO - You only look once YOLO!!! So do we only live once? I sure do not know. What I know is that we only have to LOOK once. Wait what? That’s right. If you want to detect and localize objects in an image, there is no need to go through the whole process of proposing regions of interest, classify them and correct their bounding boxes. If you recall from my previous article, this is exactly what models like RCNN and Faster RCNN do. Do we really need all that complexity and computation? Well if we want top-notch accuracy we certainly do. Luckily there is another simpler way to perform such a task, by processing the image only once and output the prediction immediately. These types of models are called Single shot detectors. https://www.youtube.com/watch?v=yQwfDxBMtXg Single shot detectors Instead of having a dedicated system to propose regions of interests, we have a set of predefined boxes to look for objects, which are forwarded to a bunch of convolutional layers to predict class scores and bounding box offsets. Then for each box we predict a number of bounding boxes with a confidence score assigned to each one, we detect one object centered in that box and we output a set of probabilities for each possible class. Once we have all that, we simply and maybe naively keep only the box with a high confidence score. And it works. With very impressive results actually. To elaborate the overall flow even better, let’s use one of the most popular single shot detectors called YOLO . You only look once (YOLO) There have been 3 versions of the model so far, with each new one improving the previous in terms of both speed and accuracy. The number of predefined cells and the number of predicted bounding boxes for each cell is defined based on the input size and the classes. In our case, we are going to use the actual numbers used to evaluate the PASCAL VOC dataset. First, we divide the image into a grid of 13x13, resulting in 169 cells in total. For every one of the cells, it predicts 5 bounding boxes (x,y,w,h) with a confidence score, it detects one object regardless the number of boxes and 20 probabilities for the 20 classes. In total, we have 169*5=845 bounding boxes and the shape of output tensor of the mode is going to be (13,13,5*5+20)= (13,13,45). The whole essence of the YOLO models is to build this (13,13,45) tensor. To accomplish that, it uses a CNN network and 2 fully connected layers to perform the actual regression. The final prediction is extracted after keeping only the bounding boxes with a high confidence score( higher than a threshold such as 0.3) https://pjreddie.com/darknet/yolo/ Because the model may output duplicate detections for the same object, we use a technique called Non-maximal suppression to remove duplicates. In a simple implementation, we sort the predictions by the confidence score and as we iterate them we keep only the first appearances of each class. As far as the actual model is concerned, the architecture is quite trivial as it consists of only convolutional and pooling layers, without any fancy tricks. We train the model using a multiple loss function, which includes a classification loss, a localization loss and a confidence loss. The most recent versions of YOLO have introduced some special tricks to improve the accuracy and reduce the training and inference time. Some examples are batch normalization, anchor boxes, dimensions clusters and others. If you want to get into more details, you should definitely check the original papers. Also to dive into code and try the YOLO models in practice, check out these two awesome repositories in Github (repo1 and repo2). The power of YOLO is not its spectacular accuracy or the very clever ideas behind it, is its superb speed, which makes it ideal for embedded systems and low-power applications. That’s why self-driving cars and surveillance cameras are its most common real-world use cases. As deep learning continues to play along with computer vision (and it will sure do), we can expect many more models to be tailored for low-power systems even if they sometimes sacrify accuracy. And dont forget the whole Internet of Things kind of thing. This is where these models really shine.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->



  <link rel="apple-touch-icon" sizes="180x180" href="">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/img/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/img/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/img/icons/site.webmanifest">
  <link rel="mask-icon" href="/assets/img/icons/safari-pinned-tab.svg?v=qA3OXqyw77" color="#5bbad5">
  <!--[if IE]><link rel="shortcut icon" href="/assets/img/icons/favicon.ico?v=qA3OXqyw77"><![endif]-->
  <link rel="shortcut icon" href="/assets/img/icons/favicon.ico">
  <meta name="apple-mobile-web-app-title" content="Sergios Karagiannakos">
  <meta name="application-name" content="Sergios Karagiannakos">
  <meta name="description" content="Machine Learning Engineer,Software Engineer,Data Scientist"/>
  <!-- <meta name="msapplication-config" content="/assets/img/icons/browserconfig.xml?v=qA3OXqyw77"> -->
  <meta name="theme-color" content="#ffffff">

  

    <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=GTM-WMXQPRS"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'GTM-WMXQPRS');
</script>


  


  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.6.1/css/all.css" integrity="sha384-gfdkjb5BdAXd+lj+gudLWI+BXq4IuLW5IT+brZEZsLFm++aCMlF1V92rMkPaX4PP" crossorigin="anonymous">
</head>


  <body class="site">

    

      <!-- Google Tag Manager (noscript) -->
      <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WMXQPRS"
      height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
      <!-- End Google Tag Manager (noscript) -->

    

    <!--/ Nav Star /-->
<nav class="navbar navbar-b navbar-trans navbar-expand-md fixed-top" id="mainNav" itemscope itemtype="http://schema.org/SiteNavigationElement" aria-label="Main navigation">
    <div class="container">
      <a class="navbar-brand js-scroll" href=""> SERGIOS KARAGIANNAKOS</a>
      <button class="navbar-toggler collapsed" type="button" data-toggle="collapse" data-target="#navbarDefault"
        aria-controls="navbarDefault" aria-expanded="false" aria-label="Toggle navigation">
        <span></span>
        <span></span>
        <span></span>
      </button>
      <div class="navbar-collapse collapse justify-content-end" id="navbarDefault">
        <ul class="navbar-nav">
          
            <li class="nav-item">
              <a class="nav-link" href="/" itemprop="url">
                  <span itemprop="name">Home</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/about" itemprop="url">
                  <span itemprop="name">About</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/portfolio" itemprop="url">
                  <span itemprop="name">Portfolio</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/blog" itemprop="url">
                  <span itemprop="name">Blog</span>
              </a>
            </li>
          
            <li class="nav-item">
              <a class="nav-link" href="/contact" itemprop="url">
                  <span itemprop="name">Contact</span>
              </a>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  <!--/ Nav End /-->

    

  <div class="hero" style="background-image: url( /assets/img/posts/yolo_app.jpg">
  
    <div class="hero__wrap">
      
      <h2 class="hero__title">YOLO - You only look once</h2>
      <p class="hero__meta">
        
        
                
        
        <span class="far fa-clock"> &nbsp May 13, 2019</span>
        <span>&nbsp &nbsp</span>
        <span class="far fa-calendar">  &nbsp 
            4 mins
         read</span>
      </p>
    </div>
  </div>

  


        
    <div class="container">
        <article class="post-content" itemprop="articleBody">

        <h1 id="yolo---you-only-look-once">YOLO - You only look once</h1>

<p>YOLO!!! So do we only live once? I sure do not know. What I know is that we only
have to LOOK once. Wait what?</p>

<p>That’s right. If you want to detect and localize objects in an image, there is
no need to go through the whole process of proposing regions of interest,
classify them and correct their bounding boxes. If you recall from my previous
<a href="https://sergioskar.github.io/Localization_and_Object_Detection/">article</a>, this
is exactly what models like RCNN and Faster RCNN do.</p>

<p>Do we really need all that complexity and computation? Well if we want top-notch
accuracy we certainly do. Luckily there is another simpler way to perform such a
task, by processing the image only once and output the prediction immediately.
These types of models are called Single shot detectors.</p>

<p><img src="/assets/img/posts/yolo_app.jpg" alt="yolo_app" /></p>

<blockquote>
  <blockquote>
    <blockquote>
      <p><a href="https://www.youtube.com/watch?v=yQwfDxBMtXg">https://www.youtube.com/watch?v=yQwfDxBMtXg</a></p>
    </blockquote>
  </blockquote>
</blockquote>

<h2 id="single-shot-detectors">Single shot detectors</h2>

<p>Instead of having a dedicated system to propose regions of interests, we have a
set of predefined boxes to look for objects, which are forwarded to a bunch of
convolutional layers to predict class scores and bounding box offsets. <strong>Then
for each box we predict a number of bounding boxes with a confidence score
assigned to each one, we detect one object centered in that box and we output a
set of probabilities for each possible class</strong>. Once we have all that, we simply
and maybe naively keep only the box with a high confidence score. And it works.
With very impressive results actually. To elaborate the overall flow even
better, let’s use one of the most popular single shot detectors called YOLO .</p>

<h2 id="you-only-look-once-yolo">You only look once (YOLO)</h2>

<p>There have been 3 versions of the model so far, with each new one improving the
previous in terms of both speed and accuracy. The number of predefined cells and
the number of predicted bounding boxes for each cell is defined based on the
input size and the classes. In our case, we are going to use the actual numbers
used to evaluate the <a href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL VOC</a>
dataset.</p>

<p>First, we divide the image into a grid of 13x13, resulting in 169 cells in total.</p>

<p>For every one of the cells, it predicts 5 bounding boxes (x,y,w,h) with a
confidence score, it detects one object regardless the number of boxes and 20
probabilities for the 20 classes.</p>

<p>In total, we have 169*5=845 bounding boxes and the shape of output tensor of the
mode is going to be (13,13,5*5+20)= (13,13,45). The whole essence of the YOLO
models is to build this (13,13,45) tensor. To accomplish that, it uses a CNN
network and 2 fully connected layers to perform the actual regression.</p>

<p>The final prediction is extracted after keeping only the bounding boxes with a
high confidence score( higher than a threshold such as 0.3)</p>

<p><img src="/assets/img/posts/yolo.jpg" alt="yolo" /></p>

<blockquote>
  <blockquote>
    <blockquote>
      <p><a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a></p>
    </blockquote>
  </blockquote>
</blockquote>

<p>Because the model may output duplicate detections for the same object, we use a
technique called <strong>Non-maximal suppression</strong> to remove duplicates. In a simple
implementation, we sort the predictions by the confidence score and as we
iterate them we keep only the first appearances of each class.</p>

<p>As far as the actual model is concerned, the architecture is quite trivial as it
consists of only convolutional and pooling layers, without any fancy tricks. We
train the model using a multiple loss function, which includes a classification
loss, a localization loss and a confidence loss.</p>

<p><img src="/assets/img/posts/yolo_architecture.jpg" alt="yolo_architecture" /></p>

<p>The most recent versions of YOLO have introduced some special tricks to improve
the accuracy and reduce the training and inference time. Some examples are batch
normalization, anchor boxes, dimensions clusters and others. If you want to get
into more details, you should definitely check the original
<a href="https://pjreddie.com/publications/">papers</a>.</p>

<p>Also to dive into code and try the YOLO models in practice, check out these two
awesome repositories in Github (<a href="https://github.com/qqwweee/keras-yolo3">repo1</a>
and <a href="https://github.com/experiencor/keras-yolo2">repo2</a>).</p>

<p>The power of YOLO is not its spectacular accuracy or the very clever ideas
behind it,  is its superb speed, which makes it ideal for embedded systems and
low-power applications. That’s why self-driving cars and surveillance cameras
are its most common real-world use cases.</p>

<p>As deep learning continues to play along with computer vision (and it will sure do), we
can expect many more models to be tailored for low-power systems even if they sometimes sacrify 
accuracy. And dont forget the whole Internet of Things kind of thing. This is where these models
really shine.</p>


        </article>
        <div class="post-content controls__inner">
        <div class="controls__item prev">
            

            <span>Previous</span>
            <a href="/Localization_and_Object_Detection/">
                <span>
                    <svg xmlns="http://www.w3.org/2000/svg" width="6" height="11">
                    <path fill="fillColor" d="M5.647 1.718c.37-.434.323-1.09-.106-1.465A1.016 1.016 0 0 0 4.095.36L.25 4.875a1.05 1.05 0 0 0 .017 1.378l3.95 4.407c.38.424 1.03.456 1.448.07a1.05 1.05 0 0 0 .07-1.468l-3.34-3.725 3.253-3.819z"/>
                    </svg>
                </span>
            Localization and Object Det...
            </a>
            
        </div>

        <div class="controls__item next">
            
        </div>
        </div>
    </div>


  


    

<section class="paralax-mf footer-paralax bg-image " >
<footer class="site-footer">
  <div class="container">
    <div class="row mb-3">
      <div class="col-md-12 text-center">
        <p>
          <a href="https://www.linkedin.com/in/sergios-karagiannakos " class="social-item"><span class="fab fa-linkedin"></span></a>
          <a href="https://www.instagram.com/sergios_krg" class="social-item"><span class="fab fa-instagram"></span></a>
          <a href="https://www.twitter.com/karsergios" class="social-item"><span class="fab fa-twitter"></span></a>
          <a href="https://www.facebook.com/sergios.karagiannakos" class="social-item"><span class="fab fa-facebook"></span></a>
          <a href="https://github.com/SergiosKar" class="social-item"><span class="fab fa-github"></span></a>
        </p>
      </div>
    </div>
    <div class="row">
        <p class="col-12 text-center footer_text">
        <span>&copy; 2019 Sergios Karagiannakos. All rights reserved.</span>
        </p>
    </div>
  </div>
</footer>
</section>
<!-- JS -->
 <!-- JavaScript Libraries -->
 <script src="/lib/jquery/jquery.min.js"></script>
 <script src="/lib/jquery/jquery-migrate.min.js"></script>
 <script src="/lib/popper/popper.min.js"></script>
 <script src="/lib/bootstrap/js/bootstrap.min.js"></script>
 <script src="/lib/easing/easing.min.js"></script>
 <script src="/lib/counterup/jquery.waypoints.min.js"></script>
 <script src="/lib/counterup/jquery.counterup.js"></script>
 <script src="/lib/lightbox/js/lightbox.min.js"></script>
 <script src="/lib/typed/typed.min.js"></script>
 <script src="/js/scripts.js"></script>
 <script src="/js/contact_form.js"></script>



  </body>

</html>



